{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install optuna\n",
        "%pip install sparsemax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Tc3NgvLqeox8",
        "outputId": "21d02b29-dfb9-4595-f848-437bc5b0a29e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.2.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.14.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.38)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (1.3.9)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Requirement already satisfied: sparsemax in /usr/local/lib/python3.11/dist-packages (0.1.9)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from sparsemax) (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->sparsemax) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->sparsemax) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->sparsemax) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->sparsemax) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->sparsemax) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->sparsemax) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->sparsemax) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->sparsemax) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->sparsemax) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->sparsemax) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->sparsemax) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->sparsemax) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->sparsemax) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->sparsemax) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->sparsemax) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->sparsemax) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->sparsemax) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->sparsemax) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->sparsemax) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->sparsemax) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->sparsemax) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sparsemax import Sparsemax\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "from sklearn.datasets import fetch_openml\n",
        "from scipy.stats import spearmanr\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import optuna\n",
        "import json\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "sparsemax = Sparsemax(dim=-1)\n",
        "\n",
        "class TabularDataset(Dataset):\n",
        "    \"\"\"Dataset for tabular data with categorical and continuous features\"\"\"\n",
        "    def __init__(self, X_cont, X_cat, y=None, cat_cardinalities=None):\n",
        "        # Continuous features\n",
        "        if X_cont is not None and len(X_cont) > 0:\n",
        "            self.X_cont = torch.tensor(X_cont, dtype=torch.float32)\n",
        "            self.has_cont = True\n",
        "        else:\n",
        "            self.has_cont = False\n",
        "\n",
        "        # Categorical features\n",
        "        if X_cat is not None and len(X_cat) > 0:\n",
        "            self.X_cat = torch.tensor(X_cat, dtype=torch.long)\n",
        "            self.has_cat = True\n",
        "        else:\n",
        "            self.has_cat = False\n",
        "\n",
        "        # Target\n",
        "        if y is not None:\n",
        "            self.y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
        "        else:\n",
        "            self.y = None\n",
        "\n",
        "        # Store cardinalities for reference\n",
        "        self.cat_cardinalities = cat_cardinalities\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.has_cont:\n",
        "            return len(self.X_cont)\n",
        "        else:\n",
        "            return len(self.X_cat)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = {}\n",
        "        if self.has_cont:\n",
        "            data['cont'] = self.X_cont[idx]\n",
        "        if self.has_cat:\n",
        "            data['cat'] = self.X_cat[idx]\n",
        "\n",
        "        if self.y is not None:\n",
        "            return data, self.y[idx]\n",
        "        else:\n",
        "            return data\n",
        "\n",
        "class CategoricalEmbeddings(nn.Module):\n",
        "    \"\"\"Entity embeddings for categorical features\"\"\"\n",
        "    def __init__(self, cardinalities, d_token):\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.ModuleList()\n",
        "        self.cardinalities = cardinalities\n",
        "\n",
        "        for cardinality in cardinalities:\n",
        "            self.embeddings.append(nn.Embedding(cardinality, d_token))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, num_cat_features)\n",
        "        batch_size = x.shape[0]\n",
        "        num_features = x.shape[1]\n",
        "\n",
        "        # Embed each categorical feature\n",
        "        embedded = []\n",
        "        for i in range(num_features):\n",
        "            embedded.append(self.embeddings[i](x[:, i]))\n",
        "\n",
        "        # Stack along feature dimension\n",
        "        return torch.stack(embedded, dim=1)  # (batch_size, num_features, d_token)\n",
        "\n",
        "class LinearEmbedding(nn.Module):\n",
        "    \"\"\"Linear embedding for continuous features\"\"\"\n",
        "    def __init__(self, num_features, d_token):\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.ModuleList([\n",
        "            nn.Linear(1, d_token) for _ in range(num_features)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, num_cont_features)\n",
        "        batch_size = x.shape[0]\n",
        "        num_features = x.shape[1]\n",
        "\n",
        "        # Embed each continuous feature\n",
        "        embedded = torch.zeros((batch_size, num_features, self.embeddings[0].out_features),\n",
        "                              device=x.device)\n",
        "\n",
        "        for i in range(num_features):\n",
        "            embedded[:, i] = self.embeddings[i](x[:, i].unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        return embedded  # (batch_size, num_features, d_token)\n",
        "\n",
        "class PiecewiseLinearEmbedding(nn.Module):\n",
        "    \"\"\"Piecewise linear embedding for continuous features\"\"\"\n",
        "    def __init__(self, num_features, d_token, num_bins=20):\n",
        "        super().__init__()\n",
        "        self.num_features = num_features\n",
        "        self.d_token = d_token\n",
        "        self.num_bins = num_bins\n",
        "\n",
        "        # Create embeddings for each feature\n",
        "        self.embeddings = nn.ModuleList([\n",
        "            nn.Linear(num_bins, d_token) for _ in range(num_features)\n",
        "        ])\n",
        "\n",
        "        # Create parameters for bin boundaries (learnable)\n",
        "        self.bin_boundaries = nn.Parameter(torch.randn(num_features, num_bins-1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, num_features)\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        # Output will contain embedded tokens for each feature\n",
        "        embedded = torch.zeros((batch_size, self.num_features, self.d_token), device=x.device)\n",
        "\n",
        "        for i in range(self.num_features):\n",
        "            # Get feature values for current feature\n",
        "            feature_values = x[:, i].unsqueeze(1)  # (batch_size, 1)\n",
        "\n",
        "            # Get sorted boundaries for this feature\n",
        "            boundaries = torch.sort(self.bin_boundaries[i]).values  # (num_bins-1)\n",
        "\n",
        "            # Calculate bin activations using cumulative distribution\n",
        "            # Start with all in the first bin\n",
        "            bin_activations = torch.ones((batch_size, self.num_bins), device=x.device)\n",
        "\n",
        "            # Update bin activations based on feature values and boundaries\n",
        "            for j in range(self.num_bins-1):\n",
        "                boundary = boundaries[j]\n",
        "                # Calculate contribution to bins based on boundary comparison\n",
        "                condition = feature_values > boundary\n",
        "                # Move activations to next bin when condition is true\n",
        "                bin_activations[:, j+1:] = torch.where(\n",
        "                    condition.expand(-1, self.num_bins-j-1),\n",
        "                    bin_activations[:, j:self.num_bins-1],\n",
        "                    bin_activations[:, j+1:]\n",
        "                )\n",
        "                bin_activations[:, j] = torch.where(\n",
        "                    condition.squeeze(1),\n",
        "                    0.0,\n",
        "                    bin_activations[:, j]\n",
        "                )\n",
        "\n",
        "            # Apply linear transformation to get embeddings\n",
        "            feature_embedding = self.embeddings[i](bin_activations)  # (batch_size, d_token)\n",
        "            embedded[:, i] = feature_embedding\n",
        "\n",
        "        return embedded  # (batch_size, num_features, d_token)\n",
        "\n",
        "# Custom attention module to capture attention weights\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        # Ensure d_model is divisible by num_heads\n",
        "        assert self.head_dim * num_heads == d_model, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        # Linear projections\n",
        "        self.q_proj = nn.Linear(d_model, d_model)\n",
        "        self.k_proj = nn.Linear(d_model, d_model)\n",
        "        self.v_proj = nn.Linear(d_model, d_model)\n",
        "        self.out_proj = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # For storing attention weights\n",
        "        self.attention_weights = None\n",
        "\n",
        "    def forward(self, query, key, value, attn_mask=None):\n",
        "        batch_size = query.shape[0]\n",
        "\n",
        "        # Linear projections\n",
        "        q = self.q_proj(query)  # (batch_size, seq_len, d_model)\n",
        "        k = self.k_proj(key)    # (batch_size, seq_len, d_model)\n",
        "        v = self.v_proj(value)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        q = q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Calculate attention scores\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "\n",
        "        # Apply mask if provided\n",
        "        if attn_mask is not None:\n",
        "            scores = scores.masked_fill(attn_mask == 0, -1e9)\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        self.attention_weights = attention_weights  # Store for later use\n",
        "\n",
        "        # Apply attention weights to values\n",
        "        out = torch.matmul(attention_weights, v)  # (batch_size, num_heads, seq_len, head_dim)\n",
        "\n",
        "        # Reshape back\n",
        "        out = out.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "\n",
        "        # Final linear projection\n",
        "        out = self.out_proj(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Custom transformer layer to capture attention weights\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, nhead)\n",
        "\n",
        "        # Feed-forward network\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        # Layer norm\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_mask=None):\n",
        "        # Self-attention\n",
        "        attn_output = self.self_attn(src, src, src, attn_mask=src_mask)\n",
        "        src = src + self.dropout1(attn_output)\n",
        "        src = self.norm1(src)\n",
        "\n",
        "        # Feed-forward network\n",
        "        ff_output = self.linear2(self.dropout(F.relu(self.linear1(src))))\n",
        "        src = src + self.dropout2(ff_output)\n",
        "        src = self.norm2(src)\n",
        "\n",
        "        return src\n",
        "\n",
        "class FTTransformer(nn.Module):\n",
        "    def __init__(self, cat_cardinalities=None, num_continuous=0, d_token=64, num_heads=8, num_layers=2,\n",
        "                 d_ffn=128, dropout=0.1, cont_embedding_type='linear', n_bins=20):\n",
        "        super().__init__()\n",
        "        self.d_token = d_token\n",
        "        self.embedding_type = cont_embedding_type\n",
        "\n",
        "        # Feature counts\n",
        "        self.num_categorical = len(cat_cardinalities) if cat_cardinalities else 0\n",
        "        self.num_continuous = num_continuous\n",
        "        self.total_features = self.num_categorical + self.num_continuous\n",
        "\n",
        "        # CLS token parameter\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_token))\n",
        "\n",
        "        # Categorical feature tokenizer\n",
        "        if self.num_categorical > 0:\n",
        "            self.cat_tokenizer = CategoricalEmbeddings(cat_cardinalities, d_token)\n",
        "\n",
        "        # Continuous feature tokenizer\n",
        "        if self.num_continuous > 0:\n",
        "            if cont_embedding_type == 'linear':\n",
        "                self.cont_tokenizer = LinearEmbedding(num_continuous, d_token)\n",
        "            elif cont_embedding_type == 'piecewise':\n",
        "                self.cont_tokenizer = PiecewiseLinearEmbedding(num_continuous, d_token, num_bins=n_bins)\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown continuous embedding type: {cont_embedding_type}\")\n",
        "\n",
        "        # Feature positional embedding\n",
        "        self.feature_pos_embedding = nn.Parameter(torch.randn(1, self.total_features, d_token))\n",
        "\n",
        "        # Custom transformer layers\n",
        "        self.transformer_layers = nn.ModuleList([\n",
        "            TransformerEncoderLayer(d_model=d_token, nhead=num_heads,\n",
        "                                   dim_feedforward=d_ffn, dropout=dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Output layer for classification (sigmoid applied in forward method)\n",
        "        self.output_layer = nn.Linear(d_token, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x['cat'].shape[0] if 'cat' in x else x['cont'].shape[0]\n",
        "\n",
        "        # Tokenize categorical features\n",
        "        if self.num_categorical > 0 and 'cat' in x:\n",
        "            cat_tokens = self.cat_tokenizer(x['cat'])  # (batch_size, num_cat, d_token)\n",
        "        else:\n",
        "            cat_tokens = None\n",
        "\n",
        "        # Tokenize continuous features\n",
        "        if self.num_continuous > 0 and 'cont' in x:\n",
        "            cont_tokens = self.cont_tokenizer(x['cont'])  # (batch_size, num_cont, d_token)\n",
        "        else:\n",
        "            cont_tokens = None\n",
        "\n",
        "        # Concatenate embeddings from different feature types\n",
        "        if cat_tokens is not None and cont_tokens is not None:\n",
        "            tokens = torch.cat([cat_tokens, cont_tokens], dim=1)  # (batch_size, total_features, d_token)\n",
        "        elif cat_tokens is not None:\n",
        "            tokens = cat_tokens\n",
        "        elif cont_tokens is not None:\n",
        "            tokens = cont_tokens\n",
        "        else:\n",
        "            raise ValueError(\"No features provided\")\n",
        "\n",
        "        # Add positional embedding\n",
        "        tokens = tokens + self.feature_pos_embedding\n",
        "\n",
        "        # Add CLS token\n",
        "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
        "        sequence = torch.cat([cls_tokens, tokens], dim=1)  # (batch_size, num_features+1, d_token)\n",
        "\n",
        "        # Apply transformer layers\n",
        "        for layer in self.transformer_layers:\n",
        "            sequence = layer(sequence)\n",
        "\n",
        "        # Use CLS token for prediction\n",
        "        cls_output = sequence[:, 0]\n",
        "\n",
        "        # Final prediction (logits) - no sigmoid here for BCEWithLogitsLoss\n",
        "        output = self.output_layer(cls_output)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def get_cls_attention(self):\n",
        "        \"\"\"Return the attention weights from CLS token to feature tokens (average over all layers)\"\"\"\n",
        "        # Average attention weights across all layers\n",
        "        cls_attention = []\n",
        "\n",
        "        for layer in self.transformer_layers:\n",
        "            # Extract CLS token attention to features\n",
        "            # layer_weights shape: (batch_size, num_heads, seq_len, seq_len)\n",
        "            if layer.self_attn.attention_weights is not None:\n",
        "                # Get attention from CLS (idx 0) to features (idx 1:)\n",
        "                layer_weights = layer.self_attn.attention_weights\n",
        "                cls_to_features = layer_weights[:, :, 0, 1:].mean(dim=1)  # Average over heads\n",
        "                cls_attention.append(cls_to_features)\n",
        "            else:\n",
        "                raise ValueError(\"Attention weights not available. Run forward first.\")\n",
        "\n",
        "        # Average over layers\n",
        "        avg_attention = torch.stack(cls_attention).mean(dim=0)\n",
        "        return avg_attention\n",
        "\n",
        "# Sparse attention variants\n",
        "class sparseMultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        # Ensure d_model is divisible by num_heads\n",
        "        assert self.head_dim * num_heads == d_model, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        # Linear projections\n",
        "        self.q_proj = nn.Linear(d_model, d_model)\n",
        "        self.k_proj = nn.Linear(d_model, d_model)\n",
        "        self.v_proj = nn.Linear(d_model, d_model)\n",
        "        self.out_proj = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # For storing attention weights\n",
        "        self.attention_weights = None\n",
        "\n",
        "    def forward(self, query, key, value, attn_mask=None):\n",
        "        batch_size = query.shape[0]\n",
        "\n",
        "        # Linear projections\n",
        "        q = self.q_proj(query)  # (batch_size, seq_len, d_model)\n",
        "        k = self.k_proj(key)    # (batch_size, seq_len, d_model)\n",
        "        v = self.v_proj(value)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        q = q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Calculate attention scores\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "\n",
        "        # Apply mask if provided\n",
        "        if attn_mask is not None:\n",
        "            scores = scores.masked_fill(attn_mask == 0, -1e9)\n",
        "\n",
        "        # Apply sparsemax to get attention weights\n",
        "        attention_weights = sparsemax(scores)\n",
        "        self.attention_weights = attention_weights  # Store for later use\n",
        "\n",
        "        # Apply attention weights to values\n",
        "        out = torch.matmul(attention_weights, v)  # (batch_size, num_heads, seq_len, head_dim)\n",
        "\n",
        "        # Reshape back\n",
        "        out = out.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "\n",
        "        # Final linear projection\n",
        "        out = self.out_proj(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class sparseTransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = sparseMultiHeadAttention(d_model, nhead)\n",
        "\n",
        "        # Feed-forward network\n",
        "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
        "\n",
        "        # Layer norm\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_mask=None):\n",
        "        # Self-attention\n",
        "        attn_output = self.self_attn(src, src, src, attn_mask=src_mask)\n",
        "        src = src + self.dropout1(attn_output)\n",
        "        src = self.norm1(src)\n",
        "\n",
        "        # Feed-forward network\n",
        "        ff_output = self.linear2(self.dropout(F.relu(self.linear1(src))))\n",
        "        src = src + self.dropout2(ff_output)\n",
        "        src = self.norm2(src)\n",
        "\n",
        "        return src\n",
        "\n",
        "class sparseFTTransformer(nn.Module):\n",
        "    def __init__(self, cat_cardinalities=None, num_continuous=0, d_token=64, num_heads=8, num_layers=2,\n",
        "                 d_ffn=128, dropout=0.1, cont_embedding_type='linear', n_bins=20):\n",
        "        super().__init__()\n",
        "        self.d_token = d_token\n",
        "        self.embedding_type = cont_embedding_type\n",
        "\n",
        "        # Feature counts\n",
        "        self.num_categorical = len(cat_cardinalities) if cat_cardinalities else 0\n",
        "        self.num_continuous = num_continuous\n",
        "        self.total_features = self.num_categorical + self.num_continuous\n",
        "\n",
        "        # CLS token parameter\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_token))\n",
        "\n",
        "        # Categorical feature tokenizer\n",
        "        if self.num_categorical > 0:\n",
        "            self.cat_tokenizer = CategoricalEmbeddings(cat_cardinalities, d_token)\n",
        "\n",
        "        # Continuous feature tokenizer\n",
        "        if self.num_continuous > 0:\n",
        "            if cont_embedding_type == 'linear':\n",
        "                self.cont_tokenizer = LinearEmbedding(num_continuous, d_token)\n",
        "            elif cont_embedding_type == 'piecewise':\n",
        "                self.cont_tokenizer = PiecewiseLinearEmbedding(num_continuous, d_token, num_bins=n_bins)\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown continuous embedding type: {cont_embedding_type}\")\n",
        "\n",
        "        # Feature positional embedding\n",
        "        self.feature_pos_embedding = nn.Parameter(torch.randn(1, self.total_features, d_token))\n",
        "\n",
        "        # Custom transformer layers\n",
        "        self.transformer_layers = nn.ModuleList([\n",
        "            sparseTransformerEncoderLayer(d_model=d_token, nhead=num_heads,\n",
        "                                   dim_feedforward=d_ffn, dropout=dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Output layer for classification\n",
        "        self.output_layer = nn.Linear(d_token, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x['cat'].shape[0] if 'cat' in x else x['cont'].shape[0]\n",
        "\n",
        "        # Tokenize categorical features\n",
        "        if self.num_categorical > 0 and 'cat' in x:\n",
        "            cat_tokens = self.cat_tokenizer(x['cat'])  # (batch_size, num_cat, d_token)\n",
        "        else:\n",
        "            cat_tokens = None\n",
        "\n",
        "        # Tokenize continuous features\n",
        "        if self.num_continuous > 0 and 'cont' in x:\n",
        "            cont_tokens = self.cont_tokenizer(x['cont'])  # (batch_size, num_cont, d_token)\n",
        "        else:\n",
        "            cont_tokens = None\n",
        "\n",
        "        # Concatenate embeddings from different feature types\n",
        "        if cat_tokens is not None and cont_tokens is not None:\n",
        "            tokens = torch.cat([cat_tokens, cont_tokens], dim=1)  # (batch_size, total_features, d_token)\n",
        "        elif cat_tokens is not None:\n",
        "            tokens = cat_tokens\n",
        "        elif cont_tokens is not None:\n",
        "            tokens = cont_tokens\n",
        "        else:\n",
        "            raise ValueError(\"No features provided\")\n",
        "\n",
        "        # Add positional embedding\n",
        "        tokens = tokens + self.feature_pos_embedding\n",
        "\n",
        "        # Add CLS token\n",
        "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
        "        sequence = torch.cat([cls_tokens, tokens], dim=1)  # (batch_size, num_features+1, d_token)\n",
        "\n",
        "        # Apply transformer layers\n",
        "        for layer in self.transformer_layers:\n",
        "            sequence = layer(sequence)\n",
        "\n",
        "        # Use CLS token for prediction\n",
        "        cls_output = sequence[:, 0]\n",
        "\n",
        "        # Final prediction (logits)\n",
        "        output = self.output_layer(cls_output)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def get_cls_attention(self):\n",
        "        \"\"\"Return the attention weights from CLS token to feature tokens (average over all layers)\"\"\"\n",
        "        # Average attention weights across all layers\n",
        "        cls_attention = []\n",
        "\n",
        "        for layer in self.transformer_layers:\n",
        "            # Extract CLS token attention to features\n",
        "            # layer_weights shape: (batch_size, num_heads, seq_len, seq_len)\n",
        "            if layer.self_attn.attention_weights is not None:\n",
        "                # Get attention from CLS (idx 0) to features (idx 1:)\n",
        "                layer_weights = layer.self_attn.attention_weights\n",
        "                cls_to_features = layer_weights[:, :, 0, 1:].mean(dim=1)  # Average over heads\n",
        "                cls_attention.append(cls_to_features)\n",
        "            else:\n",
        "                raise ValueError(\"Attention weights not available. Run forward first.\")\n",
        "\n",
        "        # Average over layers\n",
        "        avg_attention = torch.stack(cls_attention).mean(dim=0)\n",
        "        return avg_attention\n",
        "\n",
        "def calculate_pfi(model, X_cont, X_cat, y, cat_cardinalities, num_permutations=5):\n",
        "    \"\"\"Calculate Permutation Feature Importance (PFI) for classification with mixed features\"\"\"\n",
        "    # Convert to PyTorch tensors\n",
        "    data = {}\n",
        "    if X_cont is not None and len(X_cont) > 0:\n",
        "        data['cont'] = torch.tensor(X_cont, dtype=torch.float32).to(device)\n",
        "        has_cont = True\n",
        "    else:\n",
        "        has_cont = False\n",
        "\n",
        "    if X_cat is not None and len(X_cat) > 0:\n",
        "        data['cat'] = torch.tensor(X_cat, dtype=torch.long).to(device)\n",
        "        has_cat = True\n",
        "    else:\n",
        "        has_cat = False\n",
        "\n",
        "    y_tensor = torch.tensor(y, dtype=torch.float32).reshape(-1, 1).to(device)\n",
        "\n",
        "    # Get baseline performance\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        baseline_preds = torch.sigmoid(model(data))\n",
        "        baseline_loss = F.binary_cross_entropy(baseline_preds, y_tensor).item()\n",
        "        baseline_preds_binary = (baseline_preds > 0.5).float()\n",
        "        baseline_accuracy = (baseline_preds_binary == y_tensor).float().mean().item()\n",
        "\n",
        "    # Calculate importance for each feature\n",
        "    importances = []\n",
        "\n",
        "    # First categorical features\n",
        "    if has_cat:\n",
        "        for feat_idx in range(X_cat.shape[1]):\n",
        "            accuracies = []\n",
        "\n",
        "            for _ in range(num_permutations):\n",
        "                # Create a permuted copy of the data\n",
        "                data_permuted = {key: val.clone() if torch.is_tensor(val) else val for key, val in data.items()}\n",
        "\n",
        "                # Permute the categorical feature\n",
        "                perm_idx = torch.randperm(X_cat.shape[0])\n",
        "                data_permuted['cat'][:, feat_idx] = data_permuted['cat'][perm_idx, feat_idx]\n",
        "\n",
        "                # Calculate loss with permuted feature\n",
        "                with torch.no_grad():\n",
        "                    perm_preds = torch.sigmoid(model(data_permuted))\n",
        "                    perm_preds_binary = (perm_preds > 0.5).float()\n",
        "                    perm_accuracy = (perm_preds_binary == y_tensor).float().mean().item()\n",
        "\n",
        "                # Feature importance is the decrease in accuracy\n",
        "                accuracies.append(baseline_accuracy - perm_accuracy)\n",
        "\n",
        "            # Average over permutations (higher = more important)\n",
        "            importances.append(np.mean(accuracies))\n",
        "\n",
        "    # Then continuous features\n",
        "    if has_cont:\n",
        "        for feat_idx in range(X_cont.shape[1]):\n",
        "            accuracies = []\n",
        "\n",
        "            for _ in range(num_permutations):\n",
        "                # Create a permuted copy of the data\n",
        "                data_permuted = {key: val.clone() if torch.is_tensor(val) else val for key, val in data.items()}\n",
        "\n",
        "                # Permute the continuous feature\n",
        "                perm_idx = torch.randperm(X_cont.shape[0])\n",
        "                data_permuted['cont'][:, feat_idx] = data_permuted['cont'][perm_idx, feat_idx]\n",
        "\n",
        "                # Calculate loss with permuted feature\n",
        "                with torch.no_grad():\n",
        "                    perm_preds = torch.sigmoid(model(data_permuted))\n",
        "                    perm_preds_binary = (perm_preds > 0.5).float()\n",
        "                    perm_accuracy = (perm_preds_binary == y_tensor).float().mean().item()\n",
        "\n",
        "                # Feature importance is the decrease in accuracy\n",
        "                accuracies.append(baseline_accuracy - perm_accuracy)\n",
        "\n",
        "            # Average over permutations (higher = more important)\n",
        "            importances.append(np.mean(accuracies))\n",
        "\n",
        "    return np.array(importances)\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs=100, early_stopping=16):\n",
        "    \"\"\"Train the model with early stopping\"\"\"\n",
        "    model.to(device)\n",
        "    best_val_loss = float('inf')\n",
        "    early_stop_counter = 0\n",
        "    best_state = None\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            # Move data to device\n",
        "            for key in X_batch:\n",
        "                X_batch[key] = X_batch[key].to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
        "            train_correct += (predictions == y_batch).sum().item()\n",
        "            train_total += y_batch.size(0)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        val_preds = []\n",
        "        val_targets = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in val_loader:\n",
        "                # Move data to device\n",
        "                for key in X_batch:\n",
        "                    X_batch[key] = X_batch[key].to(device)\n",
        "                y_batch = y_batch.to(device)\n",
        "\n",
        "                outputs = model(X_batch)\n",
        "                loss = criterion(outputs, y_batch)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # Calculate accuracy\n",
        "                predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
        "                val_correct += (predictions == y_batch).sum().item()\n",
        "                val_total += y_batch.size(0)\n",
        "\n",
        "                # Store predictions and targets for metrics\n",
        "                val_preds.append(torch.sigmoid(outputs).cpu())\n",
        "                val_targets.append(y_batch.cpu())\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        train_accuracy = train_correct / train_total\n",
        "        val_loss /= len(val_loader)\n",
        "        val_accuracy = val_correct / val_total\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            early_stop_counter = 0\n",
        "            # Save best model state dict\n",
        "            best_state = model.state_dict()\n",
        "        else:\n",
        "            early_stop_counter += 1\n",
        "            if early_stop_counter >= early_stopping:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    # Load best model\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, X_cont, X_cat, y, device):\n",
        "    \"\"\"Evaluate model performance for classification with mixed features\"\"\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Prepare data\n",
        "    data = {}\n",
        "    if X_cont is not None and len(X_cont) > 0:\n",
        "        data['cont'] = torch.tensor(X_cont, dtype=torch.float32).to(device)\n",
        "\n",
        "    if X_cat is not None and len(X_cat) > 0:\n",
        "        data['cat'] = torch.tensor(X_cat, dtype=torch.long).to(device)\n",
        "\n",
        "    y_tensor = torch.tensor(y, dtype=torch.float32).reshape(-1, 1).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(data)\n",
        "        y_pred_proba = torch.sigmoid(logits).cpu().numpy()\n",
        "        y_pred = (y_pred_proba > 0.5).astype(int)\n",
        "\n",
        "    y = y.reshape(-1, 1)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y, y_pred)\n",
        "    precision = precision_score(y, y_pred)\n",
        "    recall = recall_score(y, y_pred)\n",
        "    f1 = f1_score(y, y_pred)\n",
        "    auc = roc_auc_score(y, y_pred_proba)\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y, y_pred)\n",
        "\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Test Precision: {precision:.4f}\")\n",
        "    print(f\"Test Recall: {recall:.4f}\")\n",
        "    print(f\"Test F1 Score: {f1:.4f}\")\n",
        "    print(f\"Test AUC: {auc:.4f}\")\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'auc': auc,\n",
        "        'confusion_matrix': cm.tolist()\n",
        "    }\n",
        "\n",
        "def analyze_pfi_attention_correlation(model, X_cont, X_cat, y, cat_feature_names, cont_feature_names, cat_cardinalities, device):\n",
        "    \"\"\"Analyze correlation between PFI and attention scores for classification with mixed features\"\"\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Prepare data for forward pass\n",
        "    data = {}\n",
        "    if X_cont is not None and len(X_cont) > 0:\n",
        "        data['cont'] = torch.tensor(X_cont, dtype=torch.float32).to(device)\n",
        "\n",
        "    if X_cat is not None and len(X_cat) > 0:\n",
        "        data['cat'] = torch.tensor(X_cat, dtype=torch.long).to(device)\n",
        "\n",
        "    # Get attention scores\n",
        "    with torch.no_grad():\n",
        "        _ = model(data)  # Forward pass to compute attention\n",
        "        attention_scores = model.get_cls_attention().cpu().numpy()\n",
        "\n",
        "    # Average attention scores across samples\n",
        "    avg_attention = attention_scores.mean(axis=0)\n",
        "\n",
        "    # Calculate PFI\n",
        "    pfi_scores = calculate_pfi(model, X_cont, X_cat, y, cat_cardinalities)\n",
        "\n",
        "    # Combine feature names in the same order as the attention scores\n",
        "    # (categorical first, then continuous)\n",
        "    feature_names = []\n",
        "    if cat_feature_names:\n",
        "        feature_names.extend(cat_feature_names)\n",
        "    if cont_feature_names:\n",
        "        feature_names.extend(cont_feature_names)\n",
        "\n",
        "    # Calculate Spearman rank correlation\n",
        "    correlation, p_value = spearmanr(pfi_scores, avg_attention)\n",
        "\n",
        "    print(f\"Spearman Rank Correlation: {correlation:.4f} (p-value: {p_value:.4f})\")\n",
        "\n",
        "    # Create a visualization\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    # Create a scatter plot with different colors for categorical and continuous features\n",
        "    scatter_cat = None\n",
        "    scatter_cont = None\n",
        "\n",
        "    if cat_feature_names and cont_feature_names:\n",
        "        n_cat = len(cat_feature_names)\n",
        "        scatter_cat = ax.scatter(pfi_scores[:n_cat], avg_attention[:n_cat], alpha=0.7,\n",
        "                                  label='Categorical Features', marker='o', color='blue')\n",
        "        scatter_cont = ax.scatter(pfi_scores[n_cat:], avg_attention[n_cat:], alpha=0.7,\n",
        "                                   label='Continuous Features', marker='x', color='red')\n",
        "    else:\n",
        "        ax.scatter(pfi_scores, avg_attention, alpha=0.7)\n",
        "\n",
        "    # Add feature labels\n",
        "    for i, name in enumerate(feature_names):\n",
        "        ax.annotate(name, (pfi_scores[i], avg_attention[i]),\n",
        "                   textcoords=\"offset points\", xytext=(0,10), ha='center', fontsize=8)\n",
        "\n",
        "    # Add best fit line\n",
        "    z = np.polyfit(pfi_scores, avg_attention, 1)\n",
        "    p = np.poly1d(z)\n",
        "    ax.plot(np.sort(pfi_scores), p(np.sort(pfi_scores)), \"r--\", alpha=0.7)\n",
        "\n",
        "    # Add correlation information\n",
        "    ax.text(0.05, 0.95, f\"Spearman ρ: {correlation:.4f}\\np-value: {p_value:.4f}\",\n",
        "            transform=ax.transAxes, verticalalignment='top',\n",
        "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "    ax.set_xlabel('Permutation Feature Importance')\n",
        "    ax.set_ylabel('CLS Token Attention Score')\n",
        "    ax.set_title('PFI vs CLS Token Attention Correlation')\n",
        "\n",
        "    if scatter_cat and scatter_cont:\n",
        "        ax.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('pfi_attention_correlation_adult.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Return results\n",
        "    results = {\n",
        "        'correlation': correlation,\n",
        "        'p_value': p_value,\n",
        "        'pfi_scores': pfi_scores.tolist(),\n",
        "        'attention_scores': avg_attention.tolist(),\n",
        "        'feature_names': feature_names\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "def load_adult_dataset():\n",
        "    \"\"\"Load and preprocess Adult dataset using fetch_openml\"\"\"\n",
        "    print(\"Loading Adult dataset...\")\n",
        "\n",
        "    # Load the Adult dataset\n",
        "    adult = fetch_openml(name='adult', version=2, as_frame=True)\n",
        "\n",
        "    # Get the data and target\n",
        "    df = adult.data\n",
        "    y = adult.target\n",
        "\n",
        "    # Convert target to binary\n",
        "    y = (y == '>50K').astype(int).values\n",
        "\n",
        "    # Identify categorical and numerical features\n",
        "    # Adult dataset has the following features:\n",
        "    categorical_features = [\n",
        "        'workclass', 'education', 'marital-status', 'occupation',\n",
        "        'relationship', 'race', 'sex', 'native-country'\n",
        "    ]\n",
        "\n",
        "    numerical_features = [\n",
        "        'age', 'capital-gain', 'capital-loss', 'hours-per-week', 'fnlwgt', 'education-num'\n",
        "    ]\n",
        "\n",
        "    # Handle missing values if any\n",
        "    for col in categorical_features:\n",
        "        if df[col].isna().any():\n",
        "            df[col] = df[col].fillna(df[col].mode()[0])\n",
        "\n",
        "    for col in numerical_features:\n",
        "        if df[col].isna().any():\n",
        "            df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=42)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Process categorical features - Label Encoding\n",
        "    label_encoders = {}\n",
        "    cat_cardinalities = []\n",
        "\n",
        "    X_cat_train = np.zeros((len(X_train), len(categorical_features)), dtype=np.int64)\n",
        "    X_cat_val = np.zeros((len(X_val), len(categorical_features)), dtype=np.int64)\n",
        "    X_cat_test = np.zeros((len(X_test), len(categorical_features)), dtype=np.int64)\n",
        "\n",
        "    for i, col in enumerate(categorical_features):\n",
        "        le = LabelEncoder()\n",
        "        X_cat_train[:, i] = le.fit_transform(X_train[col])\n",
        "        X_cat_val[:, i] = le.transform(X_val[col])\n",
        "        X_cat_test[:, i] = le.transform(X_test[col])\n",
        "\n",
        "        label_encoders[col] = le\n",
        "        cat_cardinalities.append(len(le.classes_))\n",
        "\n",
        "    # Process numerical features - Standardization\n",
        "    scaler = StandardScaler()\n",
        "    X_cont_train = scaler.fit_transform(X_train[numerical_features])\n",
        "    X_cont_val = scaler.transform(X_val[numerical_features])\n",
        "    X_cont_test = scaler.transform(X_test[numerical_features])\n",
        "\n",
        "    print(f\"Dataset shapes:\")\n",
        "    print(f\"  X_cat_train: {X_cat_train.shape}, X_cont_train: {X_cont_train.shape}\")\n",
        "    print(f\"  X_cat_val: {X_cat_val.shape}, X_cont_val: {X_cont_val.shape}\")\n",
        "    print(f\"  X_cat_test: {X_cat_test.shape}, X_cont_test: {X_cont_test.shape}\")\n",
        "    print(f\"Categorical feature cardinalities: {cat_cardinalities}\")\n",
        "\n",
        "    return (X_cont_train, X_cat_train, X_cont_val, X_cat_val, X_cont_test, X_cat_test,\n",
        "            y_train, y_val, y_test, categorical_features, numerical_features, cat_cardinalities)\n",
        "\n",
        "def tune_hyperparameters(X_cont_train, X_cat_train, y_train, X_cont_val, X_cat_val, y_val,\n",
        "                        cat_cardinalities, cont_embedding_type='linear', n_trials=20, sparse=False):\n",
        "    \"\"\"Tune hyperparameters using Optuna for mixed feature types\"\"\"\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = TabularDataset(X_cont_train, X_cat_train, y_train, cat_cardinalities)\n",
        "    val_dataset = TabularDataset(X_cont_val, X_cat_val, y_val, cat_cardinalities)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=128)\n",
        "\n",
        "    num_categorical = len(cat_cardinalities) if cat_cardinalities else 0\n",
        "    num_continuous = X_cont_train.shape[1] if X_cont_train is not None else 0\n",
        "\n",
        "    def objective(trial):\n",
        "        # Define hyperparameters to tune\n",
        "        d_token = trial.suggest_int('d_token', 32, 128)\n",
        "        num_heads = trial.suggest_int('num_heads', 2, 8)\n",
        "        num_layers = trial.suggest_int('num_layers', 1, 3)\n",
        "        d_ffn = trial.suggest_int('d_ffn', 64, 256)\n",
        "        lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
        "        dropout = trial.suggest_float('dropout', 0.0, 0.5)\n",
        "        n_bins = trial.suggest_int('n_bins', 10, 100)\n",
        "\n",
        "        # Ensure d_token is divisible by num_heads\n",
        "        d_token = (d_token // num_heads) * num_heads\n",
        "\n",
        "        # Create model with trial hyperparameters\n",
        "        if not sparse:\n",
        "            model = FTTransformer(\n",
        "                cat_cardinalities=cat_cardinalities,\n",
        "                num_continuous=num_continuous,\n",
        "                d_token=d_token,\n",
        "                num_heads=num_heads,\n",
        "                num_layers=num_layers,\n",
        "                d_ffn=d_ffn,\n",
        "                dropout=dropout,\n",
        "                cont_embedding_type=cont_embedding_type,\n",
        "                n_bins=n_bins\n",
        "            )\n",
        "        else:\n",
        "            model = sparseFTTransformer(\n",
        "                cat_cardinalities=cat_cardinalities,\n",
        "                num_continuous=num_continuous,\n",
        "                d_token=d_token,\n",
        "                num_heads=num_heads,\n",
        "                num_layers=num_layers,\n",
        "                d_ffn=d_ffn,\n",
        "                dropout=dropout,\n",
        "                cont_embedding_type=cont_embedding_type,\n",
        "                n_bins=n_bins\n",
        "            )\n",
        "\n",
        "        # Define criterion and optimizer for binary classification\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "\n",
        "        # Train for a few epochs\n",
        "        model.to(device)\n",
        "        best_val_loss = float('inf')\n",
        "\n",
        "        patience = 5\n",
        "        patience_counter = 0\n",
        "        num_epochs = 20\n",
        "\n",
        "        # Short training loop for hyperparameter search\n",
        "        for epoch in range(num_epochs):\n",
        "            # Training\n",
        "            model.train()\n",
        "            for X_batch, y_batch in train_loader:\n",
        "                # Move data to device\n",
        "                for key in X_batch:\n",
        "                    X_batch[key] = X_batch[key].to(device)\n",
        "                y_batch = y_batch.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(X_batch)\n",
        "                loss = criterion(outputs, y_batch)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            # Validation\n",
        "            model.eval()\n",
        "            val_loss = 0\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for X_batch, y_batch in val_loader:\n",
        "                    # Move data to device\n",
        "                    for key in X_batch:\n",
        "                        X_batch[key] = X_batch[key].to(device)\n",
        "                    y_batch = y_batch.to(device)\n",
        "\n",
        "                    outputs = model(X_batch)\n",
        "                    loss = criterion(outputs, y_batch)\n",
        "                    val_loss += loss.item()\n",
        "\n",
        "                    # Calculate accuracy\n",
        "                    predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
        "                    val_correct += (predictions == y_batch).sum().item()\n",
        "                    val_total += y_batch.size(0)\n",
        "\n",
        "            val_loss /= len(val_loader)\n",
        "            val_accuracy = val_correct / val_total\n",
        "\n",
        "            # Update best validation loss\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter > patience:\n",
        "                    break\n",
        "\n",
        "\n",
        "        return best_val_loss\n",
        "\n",
        "    # Create Optuna study\n",
        "    study = optuna.create_study(direction=\"minimize\")\n",
        "    study.optimize(objective, n_trials=n_trials, timeout=1800)\n",
        "\n",
        "    # Print best parameters\n",
        "    print(\"Best trial:\")\n",
        "    trial = study.best_trial\n",
        "    print(f\"  Value (validation loss): {trial.value:.4f}\")\n",
        "    print(\"  Params:\")\n",
        "    for key, value in trial.params.items():\n",
        "        print(f\"    {key}: {value}\")\n",
        "\n",
        "    # Return best parameters\n",
        "    return trial.params\n",
        "\n",
        "def train_with_best_params(X_cont_train, X_cat_train, y_train, X_cont_val, X_cat_val, y_val,\n",
        "                          X_cont_test, X_cat_test, y_test, cat_cardinalities, best_params,\n",
        "                          cont_embedding_type='linear', sparse=False):\n",
        "    \"\"\"Train a model with the best hyperparameters for mixed feature types\"\"\"\n",
        "    # Create datasets\n",
        "    train_dataset = TabularDataset(X_cont_train, X_cat_train, y_train, cat_cardinalities)\n",
        "    val_dataset = TabularDataset(X_cont_val, X_cat_val, y_val, cat_cardinalities)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=1024)\n",
        "\n",
        "    # Ensure d_token is divisible by num_heads\n",
        "    d_token = (best_params['d_token'] // best_params['num_heads']) * best_params['num_heads']\n",
        "\n",
        "    num_continuous = X_cont_train.shape[1] if X_cont_train is not None else 0\n",
        "\n",
        "    # Create model with best hyperparameters\n",
        "    if not sparse:\n",
        "        model = FTTransformer(\n",
        "            cat_cardinalities=cat_cardinalities,\n",
        "            num_continuous=num_continuous,\n",
        "            d_token=d_token,\n",
        "            num_heads=best_params['num_heads'],\n",
        "            num_layers=best_params['num_layers'],\n",
        "            d_ffn=best_params['d_ffn'],\n",
        "            dropout=best_params['dropout'],\n",
        "            cont_embedding_type=cont_embedding_type,\n",
        "            n_bins=best_params['n_bins']\n",
        "        )\n",
        "    else:\n",
        "        model = sparseFTTransformer(\n",
        "            cat_cardinalities=cat_cardinalities,\n",
        "            num_continuous=num_continuous,\n",
        "            d_token=d_token,\n",
        "            num_heads=best_params['num_heads'],\n",
        "            num_layers=best_params['num_layers'],\n",
        "            d_ffn=best_params['d_ffn'],\n",
        "            dropout=best_params['dropout'],\n",
        "            cont_embedding_type=cont_embedding_type,\n",
        "            n_bins=best_params['n_bins']\n",
        "        )\n",
        "\n",
        "    # Define criterion for binary classification\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=best_params['lr'], weight_decay=1e-5)\n",
        "\n",
        "    # Train the model with early stopping\n",
        "    model = train_model(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        criterion=criterion,\n",
        "        optimizer=optimizer,\n",
        "        device=device,\n",
        "        epochs=100\n",
        "    )\n",
        "\n",
        "    # Evaluate on test set\n",
        "    results = evaluate_model(model, X_cont_test, X_cat_test, y_test, device)\n",
        "\n",
        "    return model, results\n",
        "\n",
        "def visualize_all_models(results, cat_feature_names, cont_feature_names):\n",
        "    \"\"\"Create a comprehensive visualization comparing all models with mixed feature types\"\"\"\n",
        "    all_feature_names = []\n",
        "    if cat_feature_names:\n",
        "        all_feature_names.extend(cat_feature_names)\n",
        "    if cont_feature_names:\n",
        "        all_feature_names.extend(cont_feature_names)\n",
        "\n",
        "    # Create a figure with 2x2 subplots\n",
        "    fig, axs = plt.subplots(2, 2, figsize=(20, 16))\n",
        "\n",
        "    # Plot performance metrics\n",
        "    models = list(results.keys())\n",
        "    acc_values = [results[model]['accuracy'] for model in models]\n",
        "    f1_values = [results[model]['f1'] for model in models]\n",
        "\n",
        "    # Accuracy comparison\n",
        "    axs[0, 0].bar(models, acc_values)\n",
        "    axs[0, 0].set_title('Accuracy Comparison (Adult)')\n",
        "    axs[0, 0].set_ylabel('Accuracy')\n",
        "    axs[0, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # F1 comparison\n",
        "    axs[0, 1].bar(models, f1_values)\n",
        "    axs[0, 1].set_title('F1 Score Comparison (Adult)')\n",
        "    axs[0, 1].set_ylabel('F1 Score')\n",
        "    axs[0, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Correlation comparison\n",
        "    correlations = [results[model]['correlation_analysis']['correlation'] for model in models]\n",
        "    p_values = [results[model]['correlation_analysis']['p_value'] for model in models]\n",
        "\n",
        "    axs[1, 0].bar(models, correlations)\n",
        "    axs[1, 0].set_title('PFI-Attention Correlation Comparison (Adult)')\n",
        "    axs[1, 0].set_ylabel('Spearman Correlation')\n",
        "    axs[1, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Feature importance comparison across models (top 3 features)\n",
        "    axs[1, 1].axis('off')  # Turn off the axis for the text summary\n",
        "\n",
        "    summary_text = \"Feature Importance Summary:\\n\\n\"\n",
        "\n",
        "    for model in models:\n",
        "        pfi_scores = np.array(results[model]['correlation_analysis']['pfi_scores'])\n",
        "        attn_scores = np.array(results[model]['correlation_analysis']['attention_scores'])\n",
        "        feature_names = results[model]['correlation_analysis']['feature_names']\n",
        "\n",
        "        # Get top 3 features by PFI\n",
        "        pfi_top_indices = np.argsort(-pfi_scores)[:3]\n",
        "        pfi_top_features = [feature_names[i] for i in pfi_top_indices]\n",
        "\n",
        "        # Get top 3 features by attention\n",
        "        attn_top_indices = np.argsort(-attn_scores)[:3]\n",
        "        attn_top_features = [feature_names[i] for i in attn_top_indices]\n",
        "\n",
        "        summary_text += f\"{model}:\\n\"\n",
        "        summary_text += f\"  Top PFI features: {', '.join(pfi_top_features)}\\n\"\n",
        "        summary_text += f\"  Top attention features: {', '.join(attn_top_features)}\\n\\n\"\n",
        "\n",
        "    axs[1, 1].text(0.05, 0.95, summary_text, transform=axs[1, 1].transAxes,\n",
        "                 verticalalignment='top', fontsize=12)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('model_comparison_adult.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Create additional visualization for feature importance comparison\n",
        "    # For top 20 features only to avoid cluttering\n",
        "    num_top_features = min(20, len(all_feature_names))\n",
        "    fig, axs = plt.subplots(len(models), 1, figsize=(14, 5 * len(models)))\n",
        "\n",
        "    if len(models) == 1:\n",
        "        axs = [axs]  # Convert to list if there's only one model\n",
        "\n",
        "    for i, model in enumerate(models):\n",
        "        pfi_scores = np.array(results[model]['correlation_analysis']['pfi_scores'])\n",
        "        attn_scores = np.array(results[model]['correlation_analysis']['attention_scores'])\n",
        "        feature_names = results[model]['correlation_analysis']['feature_names']\n",
        "\n",
        "        # Sort features by PFI for visualization (top 20)\n",
        "        sorted_indices = np.argsort(-pfi_scores)[:num_top_features]\n",
        "        sorted_features = [feature_names[j] for j in sorted_indices]\n",
        "        sorted_pfi = [pfi_scores[j] for j in sorted_indices]\n",
        "        sorted_attn = [attn_scores[j] for j in sorted_indices]\n",
        "\n",
        "        x = np.arange(len(sorted_features))\n",
        "        width = 0.35\n",
        "\n",
        "        # Color-code categorical vs. continuous features\n",
        "        colors_pfi = []\n",
        "        colors_attn = []\n",
        "        n_cat = len(cat_feature_names) if cat_feature_names else 0\n",
        "\n",
        "        for feat in sorted_features:\n",
        "            if feat in cat_feature_names:\n",
        "                colors_pfi.append('blue')\n",
        "                colors_attn.append('lightblue')\n",
        "            else:\n",
        "                colors_pfi.append('red')\n",
        "                colors_attn.append('lightcoral')\n",
        "\n",
        "        axs[i].bar(x - width/2, sorted_pfi, width, label='PFI', color=colors_pfi)\n",
        "        axs[i].bar(x + width/2, sorted_attn, width, label='Attention', color=colors_attn)\n",
        "\n",
        "        axs[i].set_title(f'Feature Importance (Adult): {model}')\n",
        "        axs[i].set_ylabel('Importance Score')\n",
        "        axs[i].set_xticks(x)\n",
        "        axs[i].set_xticklabels(sorted_features, rotation=45, ha='right')\n",
        "\n",
        "        # Add legend with categorical/continuous indicators\n",
        "        from matplotlib.patches import Patch\n",
        "        legend_elements = [\n",
        "            Patch(facecolor='blue', label='Cat. PFI'),\n",
        "            Patch(facecolor='lightblue', label='Cat. Attention'),\n",
        "            Patch(facecolor='red', label='Cont. PFI'),\n",
        "            Patch(facecolor='lightcoral', label='Cont. Attention')\n",
        "        ]\n",
        "        axs[i].legend(handles=legend_elements)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('feature_importance_comparison_adult.png')\n",
        "    plt.close()\n",
        "\n",
        "    print(\"\\nVisualizations saved as 'model_comparison_adult.png' and 'feature_importance_comparison_adult.png'\")\n",
        "\n",
        "def save_model(model, filename):\n",
        "    torch.save(model.state_dict(), filename)\n",
        "    print(f\"Model saved as {filename}\")\n",
        "\n",
        "def save_results(results, filename):\n",
        "    # Convert numpy arrays to lists for json serialization\n",
        "    for model in results:\n",
        "        if 'correlation_analysis' in results[model]:\n",
        "            if isinstance(results[model]['correlation_analysis']['pfi_scores'], np.ndarray):\n",
        "                results[model]['correlation_analysis']['pfi_scores'] = results[model]['correlation_analysis']['pfi_scores'].tolist()\n",
        "            if isinstance(results[model]['correlation_analysis']['attention_scores'], np.ndarray):\n",
        "                results[model]['correlation_analysis']['attention_scores'] = results[model]['correlation_analysis']['attention_scores'].tolist()\n",
        "\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "    print(f\"Results saved as {filename}\")\n",
        "\n",
        "def main_with_tuning():\n",
        "    \"\"\"Main function to run the Adult dataset experiments with categorical and continuous features\"\"\"\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Set device\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load Adult dataset with proper feature separation\n",
        "    (X_cont_train, X_cat_train, X_cont_val, X_cat_val, X_cont_test, X_cat_test,\n",
        "     y_train, y_val, y_test, categorical_features, numerical_features,\n",
        "     cat_cardinalities) = load_adult_dataset()\n",
        "\n",
        "    models = {}\n",
        "    results = {}\n",
        "\n",
        "    # Tune hyperparameters for Linear Embedding\n",
        "    print(\"\\n=== Tuning Hyperparameters for FT Transformer with Linear Embedding ===\")\n",
        "    linear_best_params = tune_hyperparameters(\n",
        "        X_cont_train=X_cont_train,\n",
        "        X_cat_train=X_cat_train,\n",
        "        y_train=y_train,\n",
        "        X_cont_val=X_cont_val,\n",
        "        X_cat_val=X_cat_val,\n",
        "        y_val=y_val,\n",
        "        cat_cardinalities=cat_cardinalities,\n",
        "        cont_embedding_type='linear',\n",
        "        n_trials=20\n",
        "    )\n",
        "\n",
        "    # Train with best parameters for Linear Embedding\n",
        "    print(\"\\n=== Training FT Transformer with Linear Embedding (Tuned) ===\")\n",
        "    ft_linear_tuned, linear_results = train_with_best_params(\n",
        "        X_cont_train=X_cont_train,\n",
        "        X_cat_train=X_cat_train,\n",
        "        y_train=y_train,\n",
        "        X_cont_val=X_cont_val,\n",
        "        X_cat_val=X_cat_val,\n",
        "        y_val=y_val,\n",
        "        X_cont_test=X_cont_test,\n",
        "        X_cat_test=X_cat_test,\n",
        "        y_test=y_test,\n",
        "        cat_cardinalities=cat_cardinalities,\n",
        "        best_params=linear_best_params,\n",
        "        cont_embedding_type='linear'\n",
        "    )\n",
        "\n",
        "    save_model(ft_linear_tuned, 'ft_linear_tuned_adult.pth')\n",
        "\n",
        "    models['ft_linear_tuned'] = ft_linear_tuned\n",
        "    results['ft_linear_tuned'] = linear_results\n",
        "\n",
        "    # Analyze PFI and attention correlation for tuned linear model\n",
        "    print(\"\\n=== Analyzing PFI vs Attention Correlation for Linear Embedding (Tuned) ===\")\n",
        "    linear_tuned_correlation = analyze_pfi_attention_correlation(\n",
        "        model=ft_linear_tuned,\n",
        "        X_cont=X_cont_val,\n",
        "        X_cat=X_cat_val,\n",
        "        y=y_val,\n",
        "        cat_feature_names=categorical_features,\n",
        "        cont_feature_names=numerical_features,\n",
        "        cat_cardinalities=cat_cardinalities,\n",
        "        device=device\n",
        "    )\n",
        "    results['ft_linear_tuned']['correlation_analysis'] = linear_tuned_correlation\n",
        "\n",
        "    # Tune hyperparameters for Piecewise Linear Embedding\n",
        "    print(\"\\n=== Tuning Hyperparameters for FT Transformer with Piecewise Linear Embedding ===\")\n",
        "    piecewise_best_params = tune_hyperparameters(\n",
        "        X_cont_train=X_cont_train,\n",
        "        X_cat_train=X_cat_train,\n",
        "        y_train=y_train,\n",
        "        X_cont_val=X_cont_val,\n",
        "        X_cat_val=X_cat_val,\n",
        "        y_val=y_val,\n",
        "        cat_cardinalities=cat_cardinalities,\n",
        "        cont_embedding_type='piecewise',\n",
        "        n_trials=20\n",
        "    )\n",
        "\n",
        "    # Train with best parameters for Piecewise Linear Embedding\n",
        "    print(\"\\n=== Training FT Transformer with Piecewise Linear Embedding (Tuned) ===\")\n",
        "    ft_piecewise_tuned, piecewise_results = train_with_best_params(\n",
        "        X_cont_train=X_cont_train,\n",
        "        X_cat_train=X_cat_train,\n",
        "        y_train=y_train,\n",
        "        X_cont_val=X_cont_val,\n",
        "        X_cat_val=X_cat_val,\n",
        "        y_val=y_val,\n",
        "        X_cont_test=X_cont_test,\n",
        "        X_cat_test=X_cat_test,\n",
        "        y_test=y_test,\n",
        "        cat_cardinalities=cat_cardinalities,\n",
        "        best_params=piecewise_best_params,\n",
        "        cont_embedding_type='piecewise'\n",
        "    )\n",
        "\n",
        "    save_model(ft_piecewise_tuned, 'ft_piecewise_tuned_adult.pth')\n",
        "\n",
        "    models['ft_piecewise_tuned'] = ft_piecewise_tuned\n",
        "    results['ft_piecewise_tuned'] = piecewise_results\n",
        "\n",
        "    # Analyze PFI and attention correlation for tuned piecewise model\n",
        "    print(\"\\n=== Analyzing PFI vs Attention Correlation for Piecewise Embedding (Tuned) ===\")\n",
        "    piecewise_tuned_correlation = analyze_pfi_attention_correlation(\n",
        "        model=ft_piecewise_tuned,\n",
        "        X_cont=X_cont_val,\n",
        "        X_cat=X_cat_val,\n",
        "        y=y_val,\n",
        "        cat_feature_names=categorical_features,\n",
        "        cont_feature_names=numerical_features,\n",
        "        cat_cardinalities=cat_cardinalities,\n",
        "        device=device\n",
        "    )\n",
        "    results['ft_piecewise_tuned']['correlation_analysis'] = piecewise_tuned_correlation\n",
        "\n",
        "    # Tune hyperparameters for Sparse Linear Embedding\n",
        "    print(\"\\n=== Tuning Hyperparameters for sparse FT Transformer with Linear Embedding ===\")\n",
        "    sparse_linear_best_params = tune_hyperparameters(\n",
        "        X_cont_train=X_cont_train,\n",
        "        X_cat_train=X_cat_train,\n",
        "        y_train=y_train,\n",
        "        X_cont_val=X_cont_val,\n",
        "        X_cat_val=X_cat_val,\n",
        "        y_val=y_val,\n",
        "        cat_cardinalities=cat_cardinalities,\n",
        "        cont_embedding_type='linear',\n",
        "        n_trials=20,\n",
        "        sparse=True\n",
        "    )\n",
        "\n",
        "    # Train with best parameters for Sparse Linear Embedding\n",
        "    print(\"\\n=== Training sparse FT Transformer with Linear Embedding (Tuned) ===\")\n",
        "    sparse_ft_linear_tuned, sparse_linear_results = train_with_best_params(\n",
        "        X_cont_train=X_cont_train,\n",
        "        X_cat_train=X_cat_train,\n",
        "        y_train=y_train,\n",
        "        X_cont_val=X_cont_val,\n",
        "        X_cat_val=X_cat_val,\n",
        "        y_val=y_val,\n",
        "        X_cont_test=X_cont_test,\n",
        "        X_cat_test=X_cat_test,\n",
        "        y_test=y_test,\n",
        "        cat_cardinalities=cat_cardinalities,\n",
        "        best_params=sparse_linear_best_params,\n",
        "        cont_embedding_type='linear',\n",
        "        sparse=True\n",
        "    )\n",
        "\n",
        "    save_model(sparse_ft_linear_tuned, 'sparse_ft_linear_tuned_adult.pth')\n",
        "\n",
        "    models['sparse_ft_linear_tuned'] = sparse_ft_linear_tuned\n",
        "    results['sparse_ft_linear_tuned'] = sparse_linear_results\n",
        "\n",
        "    # Analyze PFI and attention correlation for tuned sparse linear model\n",
        "    print(\"\\n=== Analyzing PFI vs Attention Correlation for Sparse Linear Embedding (Tuned) ===\")\n",
        "    sparse_linear_tuned_correlation = analyze_pfi_attention_correlation(\n",
        "        model=sparse_ft_linear_tuned,\n",
        "        X_cont=X_cont_val,\n",
        "        X_cat=X_cat_val,\n",
        "        y=y_val,\n",
        "        cat_feature_names=categorical_features,\n",
        "        cont_feature_names=numerical_features,\n",
        "        cat_cardinalities=cat_cardinalities,\n",
        "        device=device\n",
        "    )\n",
        "    results['sparse_ft_linear_tuned']['correlation_analysis'] = sparse_linear_tuned_correlation\n",
        "\n",
        "    # Tune hyperparameters for Sparse Piecewise Embedding\n",
        "    print(\"\\n=== Tuning Hyperparameters for sparse FT Transformer with Piecewise Linear Embedding ===\")\n",
        "    sparse_piecewise_best_params = tune_hyperparameters(\n",
        "        X_cont_train=X_cont_train,\n",
        "        X_cat_train=X_cat_train,\n",
        "        y_train=y_train,\n",
        "        X_cont_val=X_cont_val,\n",
        "        X_cat_val=X_cat_val,\n",
        "        y_val=y_val,\n",
        "        cat_cardinalities=cat_cardinalities,\n",
        "        cont_embedding_type='piecewise',\n",
        "        n_trials=20,\n",
        "        sparse=True\n",
        "    )\n",
        "\n",
        "    # Train with best parameters for Sparse Piecewise Embedding\n",
        "    print(\"\\n=== Training sparse FT Transformer with Piecewise Linear Embedding (Tuned) ===\")\n",
        "    sparse_ft_piecewise_tuned, sparse_piecewise_results = train_with_best_params(\n",
        "        X_cont_train=X_cont_train,\n",
        "        X_cat_train=X_cat_train,\n",
        "        y_train=y_train,\n",
        "        X_cont_val=X_cont_val,\n",
        "        X_cat_val=X_cat_val,\n",
        "        y_val=y_val,\n",
        "        X_cont_test=X_cont_test,\n",
        "        X_cat_test=X_cat_test,\n",
        "        y_test=y_test,\n",
        "        cat_cardinalities=cat_cardinalities,\n",
        "        best_params=sparse_piecewise_best_params,\n",
        "        cont_embedding_type='piecewise',\n",
        "        sparse=True\n",
        "    )\n",
        "\n",
        "    save_model(sparse_ft_piecewise_tuned, 'sparse_ft_piecewise_tuned_adult.pth')\n",
        "\n",
        "    models['sparse_ft_piecewise_tuned'] = sparse_ft_piecewise_tuned\n",
        "    results['sparse_ft_piecewise_tuned'] = sparse_piecewise_results\n",
        "\n",
        "    # Analyze PFI and attention correlation for tuned sparse piecewise model\n",
        "    print(\"\\n=== Analyzing PFI vs Attention Correlation for Sparse Piecewise Embedding (Tuned) ===\")\n",
        "    sparse_piecewise_tuned_correlation = analyze_pfi_attention_correlation(\n",
        "        model=sparse_ft_piecewise_tuned,\n",
        "        X_cont=X_cont_val,\n",
        "        X_cat=X_cat_val,\n",
        "        y=y_val,\n",
        "        cat_feature_names=categorical_features,\n",
        "        cont_feature_names=numerical_features,\n",
        "        cat_cardinalities=cat_cardinalities,\n",
        "        device=device\n",
        "    )\n",
        "    results['sparse_ft_piecewise_tuned']['correlation_analysis'] = sparse_piecewise_tuned_correlation\n",
        "\n",
        "    # Compare the results\n",
        "    print(\"\\n=== Comparison of Tuned Models ===\")\n",
        "    print(f\"FT Transformer (Linear Tuned): Accuracy={results['ft_linear_tuned']['accuracy']:.4f}, F1={results['ft_linear_tuned']['f1']:.4f}\")\n",
        "    print(f\"FT Transformer (Piecewise Tuned): Accuracy={results['ft_piecewise_tuned']['accuracy']:.4f}, F1={results['ft_piecewise_tuned']['f1']:.4f}\")\n",
        "    print(f\"Sparse FT Transformer (Linear Tuned): Accuracy={results['sparse_ft_linear_tuned']['accuracy']:.4f}, F1={results['sparse_ft_linear_tuned']['f1']:.4f}\")\n",
        "    print(f\"Sparse FT Transformer (Piecewise Tuned): Accuracy={results['sparse_ft_piecewise_tuned']['accuracy']:.4f}, F1={results['sparse_ft_piecewise_tuned']['f1']:.4f}\")\n",
        "\n",
        "    print(\"\\n=== Comparison of PFI-Attention Correlations (Tuned Models) ===\")\n",
        "    print(f\"FT Transformer (Linear Tuned): ρ={linear_tuned_correlation['correlation']:.4f}, p-value={linear_tuned_correlation['p_value']:.4f}\")\n",
        "    print(f\"FT Transformer (Piecewise Tuned): ρ={piecewise_tuned_correlation['correlation']:.4f}, p-value={piecewise_tuned_correlation['p_value']:.4f}\")\n",
        "    print(f\"Sparse FT Transformer (Linear Tuned): ρ={sparse_linear_tuned_correlation['correlation']:.4f}, p-value={sparse_linear_tuned_correlation['p_value']:.4f}\")\n",
        "    print(f\"Sparse FT Transformer (Piecewise Tuned): ρ={sparse_piecewise_tuned_correlation['correlation']:.4f}, p-value={sparse_piecewise_tuned_correlation['p_value']:.4f}\")\n",
        "\n",
        "    # Create visualization comparing all models\n",
        "    visualize_all_models(\n",
        "        results=results,\n",
        "        cat_feature_names=categorical_features,\n",
        "        cont_feature_names=numerical_features\n",
        "    )\n",
        "\n",
        "    save_results(results, 'results_adult.json')\n",
        "\n",
        "    return models, results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_with_tuning()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoLqkxBXcJLr",
        "outputId": "067539a1-6ade-4093-82f2-76c89c6b0fd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading Adult dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-0437500be3f4>:884: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[col] = df[col].fillna(df[col].mode()[0])\n",
            "[I 2025-03-04 15:57:10,160] A new study created in memory with name: no-name-a77e7a77-88a8-42ba-b79c-d88f0b7629bc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shapes:\n",
            "  X_cat_train: (31258, 8), X_cont_train: (31258, 6)\n",
            "  X_cat_val: (7815, 8), X_cont_val: (7815, 6)\n",
            "  X_cat_test: (9769, 8), X_cont_test: (9769, 6)\n",
            "Categorical feature cardinalities: [8, 16, 7, 14, 6, 5, 2, 41]\n",
            "\n",
            "=== Tuning Hyperparameters for FT Transformer with Linear Embedding ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-04 15:58:10,796] Trial 0 finished with value: 0.31081729334208275 and parameters: {'d_token': 41, 'num_heads': 8, 'num_layers': 1, 'd_ffn': 106, 'lr': 0.0003410999348349744, 'dropout': 0.44689283934342133, 'n_bins': 36}. Best is trial 0 with value: 0.31081729334208275.\n",
            "[I 2025-03-04 15:58:51,299] Trial 1 finished with value: 0.3239999077012462 and parameters: {'d_token': 69, 'num_heads': 5, 'num_layers': 2, 'd_ffn': 205, 'lr': 0.0031273098322414507, 'dropout': 0.029989994049630153, 'n_bins': 68}. Best is trial 0 with value: 0.31081729334208275.\n",
            "[I 2025-03-04 15:59:40,620] Trial 2 finished with value: 0.31477202979787705 and parameters: {'d_token': 123, 'num_heads': 2, 'num_layers': 2, 'd_ffn': 166, 'lr': 0.0009762856449216884, 'dropout': 0.43636111200584454, 'n_bins': 96}. Best is trial 0 with value: 0.31081729334208275.\n",
            "[I 2025-03-04 16:00:52,199] Trial 3 finished with value: 0.31045347596368483 and parameters: {'d_token': 78, 'num_heads': 2, 'num_layers': 1, 'd_ffn': 195, 'lr': 0.003271597916847128, 'dropout': 0.06845025976079488, 'n_bins': 94}. Best is trial 3 with value: 0.31045347596368483.\n",
            "[I 2025-03-04 16:02:00,523] Trial 4 finished with value: 0.3113339807718031 and parameters: {'d_token': 101, 'num_heads': 7, 'num_layers': 3, 'd_ffn': 143, 'lr': 0.0005483837194011757, 'dropout': 0.4623730633086879, 'n_bins': 81}. Best is trial 3 with value: 0.31045347596368483.\n",
            "[I 2025-03-04 16:03:04,439] Trial 5 finished with value: 0.31166719669295895 and parameters: {'d_token': 109, 'num_heads': 2, 'num_layers': 2, 'd_ffn': 158, 'lr': 0.0006345331984785864, 'dropout': 0.022873379950367756, 'n_bins': 20}. Best is trial 3 with value: 0.31045347596368483.\n",
            "[I 2025-03-04 16:04:19,699] Trial 6 finished with value: 0.30880800490417787 and parameters: {'d_token': 45, 'num_heads': 6, 'num_layers': 1, 'd_ffn': 127, 'lr': 0.00014403790457055048, 'dropout': 0.28454893748172727, 'n_bins': 69}. Best is trial 6 with value: 0.30880800490417787.\n",
            "[I 2025-03-04 16:05:35,209] Trial 7 finished with value: 0.3300489578996935 and parameters: {'d_token': 50, 'num_heads': 5, 'num_layers': 1, 'd_ffn': 147, 'lr': 0.008765082208371707, 'dropout': 0.2817381135264306, 'n_bins': 94}. Best is trial 6 with value: 0.30880800490417787.\n",
            "[I 2025-03-04 16:06:44,725] Trial 8 finished with value: 0.31195250081439174 and parameters: {'d_token': 101, 'num_heads': 6, 'num_layers': 3, 'd_ffn': 125, 'lr': 0.001294240011603243, 'dropout': 0.38291819161096075, 'n_bins': 63}. Best is trial 6 with value: 0.30880800490417787.\n",
            "[I 2025-03-04 16:08:26,770] Trial 9 finished with value: 0.3071337496080706 and parameters: {'d_token': 127, 'num_heads': 5, 'num_layers': 3, 'd_ffn': 182, 'lr': 0.0002671201073184819, 'dropout': 0.045727100194137504, 'n_bins': 55}. Best is trial 9 with value: 0.3071337496080706.\n",
            "[I 2025-03-04 16:09:47,027] Trial 10 finished with value: 0.3069771920000353 and parameters: {'d_token': 125, 'num_heads': 4, 'num_layers': 3, 'd_ffn': 245, 'lr': 0.00010621602107347005, 'dropout': 0.1624405010835594, 'n_bins': 41}. Best is trial 10 with value: 0.3069771920000353.\n",
            "[I 2025-03-04 16:11:48,811] Trial 11 finished with value: 0.3026639802321311 and parameters: {'d_token': 126, 'num_heads': 4, 'num_layers': 3, 'd_ffn': 252, 'lr': 0.00010993775924983645, 'dropout': 0.13820584848558642, 'n_bins': 41}. Best is trial 11 with value: 0.3026639802321311.\n",
            "[I 2025-03-04 16:13:50,851] Trial 12 finished with value: 0.3048810160929157 and parameters: {'d_token': 114, 'num_heads': 4, 'num_layers': 3, 'd_ffn': 256, 'lr': 0.00010088954028842015, 'dropout': 0.16008856898266252, 'n_bins': 37}. Best is trial 11 with value: 0.3026639802321311.\n",
            "[I 2025-03-04 16:15:53,648] Trial 13 finished with value: 0.30455135361802194 and parameters: {'d_token': 107, 'num_heads': 3, 'num_layers': 3, 'd_ffn': 256, 'lr': 0.00019259770342480834, 'dropout': 0.16382387998057546, 'n_bins': 13}. Best is trial 11 with value: 0.3026639802321311.\n",
            "[I 2025-03-04 16:17:51,449] Trial 14 finished with value: 0.30572514668587714 and parameters: {'d_token': 87, 'num_heads': 3, 'num_layers': 3, 'd_ffn': 226, 'lr': 0.00021990412479958336, 'dropout': 0.1553924093422957, 'n_bins': 11}. Best is trial 11 with value: 0.3026639802321311.\n",
            "[I 2025-03-04 16:19:31,631] Trial 15 finished with value: 0.310003679365881 and parameters: {'d_token': 94, 'num_heads': 3, 'num_layers': 2, 'd_ffn': 223, 'lr': 0.00019254561908465682, 'dropout': 0.21337741019548556, 'n_bins': 25}. Best is trial 11 with value: 0.3026639802321311.\n",
            "[I 2025-03-04 16:21:01,339] Trial 16 finished with value: 0.30631329047103084 and parameters: {'d_token': 112, 'num_heads': 3, 'num_layers': 2, 'd_ffn': 74, 'lr': 0.00037739550376730813, 'dropout': 0.1075863349895973, 'n_bins': 48}. Best is trial 11 with value: 0.3026639802321311.\n",
            "[I 2025-03-04 16:23:03,670] Trial 17 finished with value: 0.30452913430429274 and parameters: {'d_token': 63, 'num_heads': 4, 'num_layers': 3, 'd_ffn': 232, 'lr': 0.00018217583481886914, 'dropout': 0.2248944751766891, 'n_bins': 10}. Best is trial 11 with value: 0.3026639802321311.\n",
            "[I 2025-03-04 16:23:46,795] Trial 18 finished with value: 0.3187051557244793 and parameters: {'d_token': 61, 'num_heads': 4, 'num_layers': 3, 'd_ffn': 224, 'lr': 0.0012616189872646387, 'dropout': 0.341476246362587, 'n_bins': 29}. Best is trial 11 with value: 0.3026639802321311.\n",
            "[I 2025-03-04 16:25:50,634] Trial 19 finished with value: 0.3060429874927767 and parameters: {'d_token': 32, 'num_heads': 4, 'num_layers': 3, 'd_ffn': 237, 'lr': 0.0005488341757014687, 'dropout': 0.2206432235599002, 'n_bins': 46}. Best is trial 11 with value: 0.3026639802321311.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best trial:\n",
            "  Value (validation loss): 0.3027\n",
            "  Params:\n",
            "    d_token: 126\n",
            "    num_heads: 4\n",
            "    num_layers: 3\n",
            "    d_ffn: 252\n",
            "    lr: 0.00010993775924983645\n",
            "    dropout: 0.13820584848558642\n",
            "    n_bins: 41\n",
            "\n",
            "=== Training FT Transformer with Linear Embedding (Tuned) ===\n",
            "Epoch 1/100, Train Loss: 0.4107, Train Acc: 0.8065, Val Loss: 0.3324, Val Acc: 0.8429\n",
            "Epoch 2/100, Train Loss: 0.3200, Train Acc: 0.8500, Val Loss: 0.3197, Val Acc: 0.8504\n",
            "Epoch 3/100, Train Loss: 0.3133, Train Acc: 0.8527, Val Loss: 0.3161, Val Acc: 0.8517\n",
            "Epoch 4/100, Train Loss: 0.3091, Train Acc: 0.8549, Val Loss: 0.3132, Val Acc: 0.8534\n",
            "Epoch 5/100, Train Loss: 0.3085, Train Acc: 0.8558, Val Loss: 0.3181, Val Acc: 0.8536\n",
            "Epoch 6/100, Train Loss: 0.3082, Train Acc: 0.8549, Val Loss: 0.3109, Val Acc: 0.8548\n",
            "Epoch 7/100, Train Loss: 0.3064, Train Acc: 0.8564, Val Loss: 0.3149, Val Acc: 0.8516\n",
            "Epoch 8/100, Train Loss: 0.3035, Train Acc: 0.8576, Val Loss: 0.3087, Val Acc: 0.8562\n",
            "Epoch 9/100, Train Loss: 0.3029, Train Acc: 0.8582, Val Loss: 0.3082, Val Acc: 0.8559\n",
            "Epoch 10/100, Train Loss: 0.3018, Train Acc: 0.8582, Val Loss: 0.3076, Val Acc: 0.8559\n",
            "Epoch 11/100, Train Loss: 0.3021, Train Acc: 0.8586, Val Loss: 0.3079, Val Acc: 0.8568\n",
            "Epoch 12/100, Train Loss: 0.2983, Train Acc: 0.8593, Val Loss: 0.3103, Val Acc: 0.8560\n",
            "Epoch 13/100, Train Loss: 0.3008, Train Acc: 0.8595, Val Loss: 0.3066, Val Acc: 0.8569\n",
            "Epoch 14/100, Train Loss: 0.2985, Train Acc: 0.8598, Val Loss: 0.3078, Val Acc: 0.8558\n",
            "Epoch 15/100, Train Loss: 0.2990, Train Acc: 0.8609, Val Loss: 0.3053, Val Acc: 0.8589\n",
            "Epoch 16/100, Train Loss: 0.2964, Train Acc: 0.8612, Val Loss: 0.3064, Val Acc: 0.8582\n",
            "Epoch 17/100, Train Loss: 0.2970, Train Acc: 0.8613, Val Loss: 0.3032, Val Acc: 0.8586\n",
            "Epoch 18/100, Train Loss: 0.2958, Train Acc: 0.8622, Val Loss: 0.3042, Val Acc: 0.8580\n",
            "Epoch 19/100, Train Loss: 0.2959, Train Acc: 0.8626, Val Loss: 0.3044, Val Acc: 0.8576\n",
            "Epoch 20/100, Train Loss: 0.2959, Train Acc: 0.8620, Val Loss: 0.3047, Val Acc: 0.8581\n",
            "Epoch 21/100, Train Loss: 0.2949, Train Acc: 0.8620, Val Loss: 0.3025, Val Acc: 0.8600\n",
            "Epoch 22/100, Train Loss: 0.2934, Train Acc: 0.8627, Val Loss: 0.3032, Val Acc: 0.8604\n",
            "Epoch 23/100, Train Loss: 0.2912, Train Acc: 0.8630, Val Loss: 0.3036, Val Acc: 0.8609\n",
            "Epoch 24/100, Train Loss: 0.2941, Train Acc: 0.8620, Val Loss: 0.3076, Val Acc: 0.8591\n",
            "Epoch 25/100, Train Loss: 0.2899, Train Acc: 0.8647, Val Loss: 0.3024, Val Acc: 0.8609\n",
            "Epoch 26/100, Train Loss: 0.2897, Train Acc: 0.8655, Val Loss: 0.3018, Val Acc: 0.8613\n",
            "Epoch 27/100, Train Loss: 0.2920, Train Acc: 0.8636, Val Loss: 0.3017, Val Acc: 0.8608\n",
            "Epoch 28/100, Train Loss: 0.2884, Train Acc: 0.8643, Val Loss: 0.3021, Val Acc: 0.8604\n",
            "Epoch 29/100, Train Loss: 0.2881, Train Acc: 0.8669, Val Loss: 0.3024, Val Acc: 0.8622\n",
            "Epoch 30/100, Train Loss: 0.2878, Train Acc: 0.8653, Val Loss: 0.3050, Val Acc: 0.8607\n",
            "Epoch 31/100, Train Loss: 0.2868, Train Acc: 0.8656, Val Loss: 0.3026, Val Acc: 0.8605\n",
            "Epoch 32/100, Train Loss: 0.2863, Train Acc: 0.8667, Val Loss: 0.3049, Val Acc: 0.8631\n",
            "Epoch 33/100, Train Loss: 0.2856, Train Acc: 0.8654, Val Loss: 0.3036, Val Acc: 0.8619\n",
            "Epoch 34/100, Train Loss: 0.2848, Train Acc: 0.8667, Val Loss: 0.3043, Val Acc: 0.8622\n",
            "Epoch 35/100, Train Loss: 0.2833, Train Acc: 0.8683, Val Loss: 0.3040, Val Acc: 0.8600\n",
            "Epoch 36/100, Train Loss: 0.2846, Train Acc: 0.8680, Val Loss: 0.3085, Val Acc: 0.8592\n",
            "Epoch 37/100, Train Loss: 0.2830, Train Acc: 0.8671, Val Loss: 0.3050, Val Acc: 0.8598\n",
            "Epoch 38/100, Train Loss: 0.2837, Train Acc: 0.8675, Val Loss: 0.3044, Val Acc: 0.8610\n",
            "Epoch 39/100, Train Loss: 0.2828, Train Acc: 0.8673, Val Loss: 0.3040, Val Acc: 0.8595\n",
            "Epoch 40/100, Train Loss: 0.2801, Train Acc: 0.8687, Val Loss: 0.3112, Val Acc: 0.8596\n",
            "Epoch 41/100, Train Loss: 0.2806, Train Acc: 0.8685, Val Loss: 0.3099, Val Acc: 0.8603\n",
            "Epoch 42/100, Train Loss: 0.2813, Train Acc: 0.8680, Val Loss: 0.3063, Val Acc: 0.8598\n",
            "Epoch 43/100, Train Loss: 0.2799, Train Acc: 0.8705, Val Loss: 0.3066, Val Acc: 0.8596\n",
            "Early stopping at epoch 43\n",
            "Test Accuracy: 0.8705\n",
            "Test Precision: 0.7572\n",
            "Test Recall: 0.6590\n",
            "Test F1 Score: 0.7046\n",
            "Test AUC: 0.9220\n",
            "Confusion Matrix:\n",
            "[[6995  484]\n",
            " [ 781 1509]]\n",
            "Model saved as ft_linear_tuned_adult.pth\n",
            "\n",
            "=== Analyzing PFI vs Attention Correlation for Linear Embedding (Tuned) ===\n",
            "Spearman Rank Correlation: 0.2659 (p-value: 0.3581)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-04 16:27:18,555] A new study created in memory with name: no-name-f279d2cf-d533-4a9f-8304-3086e39f3fec\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Tuning Hyperparameters for FT Transformer with Piecewise Linear Embedding ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-04 16:40:20,936] Trial 0 finished with value: 0.3135595581223888 and parameters: {'d_token': 66, 'num_heads': 6, 'num_layers': 1, 'd_ffn': 196, 'lr': 0.00211989979582135, 'dropout': 0.327648253357369, 'n_bins': 95}. Best is trial 0 with value: 0.3135595581223888.\n",
            "[I 2025-03-04 16:44:01,404] Trial 1 finished with value: 0.31278201288753943 and parameters: {'d_token': 61, 'num_heads': 2, 'num_layers': 3, 'd_ffn': 172, 'lr': 0.0012646323284621608, 'dropout': 0.1171805304489657, 'n_bins': 46}. Best is trial 1 with value: 0.31278201288753943.\n",
            "[I 2025-03-04 16:48:59,331] Trial 2 finished with value: 0.3102758596501043 and parameters: {'d_token': 33, 'num_heads': 7, 'num_layers': 2, 'd_ffn': 197, 'lr': 0.0008889506767302052, 'dropout': 0.319646579393521, 'n_bins': 41}. Best is trial 2 with value: 0.3102758596501043.\n",
            "[I 2025-03-04 17:01:57,149] Trial 3 finished with value: 0.30556235678734317 and parameters: {'d_token': 75, 'num_heads': 5, 'num_layers': 3, 'd_ffn': 67, 'lr': 0.00014649590787446879, 'dropout': 0.21973072611445804, 'n_bins': 83}. Best is trial 3 with value: 0.30556235678734317.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best trial:\n",
            "  Value (validation loss): 0.3056\n",
            "  Params:\n",
            "    d_token: 75\n",
            "    num_heads: 5\n",
            "    num_layers: 3\n",
            "    d_ffn: 67\n",
            "    lr: 0.00014649590787446879\n",
            "    dropout: 0.21973072611445804\n",
            "    n_bins: 83\n",
            "\n",
            "=== Training FT Transformer with Piecewise Linear Embedding (Tuned) ===\n",
            "Epoch 1/100, Train Loss: 0.4585, Train Acc: 0.7857, Val Loss: 0.3633, Val Acc: 0.8294\n",
            "Epoch 2/100, Train Loss: 0.3408, Train Acc: 0.8400, Val Loss: 0.3343, Val Acc: 0.8448\n",
            "Epoch 3/100, Train Loss: 0.3227, Train Acc: 0.8492, Val Loss: 0.3213, Val Acc: 0.8505\n",
            "Epoch 4/100, Train Loss: 0.3144, Train Acc: 0.8526, Val Loss: 0.3194, Val Acc: 0.8514\n",
            "Epoch 5/100, Train Loss: 0.3117, Train Acc: 0.8526, Val Loss: 0.3226, Val Acc: 0.8513\n",
            "Epoch 6/100, Train Loss: 0.3094, Train Acc: 0.8557, Val Loss: 0.3138, Val Acc: 0.8537\n",
            "Epoch 7/100, Train Loss: 0.3092, Train Acc: 0.8557, Val Loss: 0.3139, Val Acc: 0.8531\n",
            "Epoch 8/100, Train Loss: 0.3093, Train Acc: 0.8573, Val Loss: 0.3121, Val Acc: 0.8540\n",
            "Epoch 9/100, Train Loss: 0.3078, Train Acc: 0.8544, Val Loss: 0.3108, Val Acc: 0.8558\n",
            "Epoch 10/100, Train Loss: 0.3072, Train Acc: 0.8561, Val Loss: 0.3111, Val Acc: 0.8548\n",
            "Epoch 11/100, Train Loss: 0.3051, Train Acc: 0.8573, Val Loss: 0.3112, Val Acc: 0.8549\n",
            "Epoch 12/100, Train Loss: 0.3038, Train Acc: 0.8581, Val Loss: 0.3097, Val Acc: 0.8560\n",
            "Epoch 13/100, Train Loss: 0.3041, Train Acc: 0.8599, Val Loss: 0.3096, Val Acc: 0.8562\n",
            "Epoch 14/100, Train Loss: 0.3033, Train Acc: 0.8570, Val Loss: 0.3099, Val Acc: 0.8557\n",
            "Epoch 15/100, Train Loss: 0.3041, Train Acc: 0.8578, Val Loss: 0.3123, Val Acc: 0.8545\n",
            "Epoch 16/100, Train Loss: 0.3006, Train Acc: 0.8577, Val Loss: 0.3134, Val Acc: 0.8567\n",
            "Epoch 17/100, Train Loss: 0.3017, Train Acc: 0.8598, Val Loss: 0.3095, Val Acc: 0.8567\n",
            "Epoch 18/100, Train Loss: 0.3014, Train Acc: 0.8595, Val Loss: 0.3097, Val Acc: 0.8560\n",
            "Epoch 19/100, Train Loss: 0.3008, Train Acc: 0.8589, Val Loss: 0.3142, Val Acc: 0.8523\n",
            "Epoch 20/100, Train Loss: 0.2998, Train Acc: 0.8603, Val Loss: 0.3074, Val Acc: 0.8576\n",
            "Epoch 21/100, Train Loss: 0.2986, Train Acc: 0.8610, Val Loss: 0.3114, Val Acc: 0.8594\n",
            "Epoch 22/100, Train Loss: 0.2984, Train Acc: 0.8603, Val Loss: 0.3094, Val Acc: 0.8560\n",
            "Epoch 23/100, Train Loss: 0.2975, Train Acc: 0.8605, Val Loss: 0.3174, Val Acc: 0.8548\n",
            "Epoch 24/100, Train Loss: 0.2983, Train Acc: 0.8604, Val Loss: 0.3109, Val Acc: 0.8581\n",
            "Epoch 25/100, Train Loss: 0.2983, Train Acc: 0.8597, Val Loss: 0.3097, Val Acc: 0.8585\n",
            "Epoch 26/100, Train Loss: 0.2976, Train Acc: 0.8613, Val Loss: 0.3055, Val Acc: 0.8590\n",
            "Epoch 27/100, Train Loss: 0.2964, Train Acc: 0.8603, Val Loss: 0.3072, Val Acc: 0.8585\n",
            "Epoch 28/100, Train Loss: 0.2968, Train Acc: 0.8620, Val Loss: 0.3084, Val Acc: 0.8580\n",
            "Epoch 29/100, Train Loss: 0.2969, Train Acc: 0.8627, Val Loss: 0.3060, Val Acc: 0.8581\n",
            "Epoch 30/100, Train Loss: 0.2960, Train Acc: 0.8615, Val Loss: 0.3077, Val Acc: 0.8566\n",
            "Epoch 31/100, Train Loss: 0.2963, Train Acc: 0.8616, Val Loss: 0.3050, Val Acc: 0.8582\n",
            "Epoch 32/100, Train Loss: 0.2947, Train Acc: 0.8626, Val Loss: 0.3058, Val Acc: 0.8605\n",
            "Epoch 33/100, Train Loss: 0.2944, Train Acc: 0.8628, Val Loss: 0.3096, Val Acc: 0.8576\n",
            "Epoch 34/100, Train Loss: 0.2939, Train Acc: 0.8626, Val Loss: 0.3058, Val Acc: 0.8581\n",
            "Epoch 35/100, Train Loss: 0.2929, Train Acc: 0.8633, Val Loss: 0.3058, Val Acc: 0.8591\n",
            "Epoch 36/100, Train Loss: 0.2939, Train Acc: 0.8637, Val Loss: 0.3047, Val Acc: 0.8603\n",
            "Epoch 37/100, Train Loss: 0.2912, Train Acc: 0.8653, Val Loss: 0.3061, Val Acc: 0.8599\n",
            "Epoch 38/100, Train Loss: 0.2926, Train Acc: 0.8645, Val Loss: 0.3034, Val Acc: 0.8607\n",
            "Epoch 39/100, Train Loss: 0.2910, Train Acc: 0.8642, Val Loss: 0.3067, Val Acc: 0.8580\n",
            "Epoch 40/100, Train Loss: 0.2908, Train Acc: 0.8637, Val Loss: 0.3027, Val Acc: 0.8613\n",
            "Epoch 41/100, Train Loss: 0.2902, Train Acc: 0.8649, Val Loss: 0.3028, Val Acc: 0.8613\n",
            "Epoch 42/100, Train Loss: 0.2909, Train Acc: 0.8655, Val Loss: 0.3036, Val Acc: 0.8608\n",
            "Epoch 43/100, Train Loss: 0.2899, Train Acc: 0.8655, Val Loss: 0.3033, Val Acc: 0.8622\n",
            "Epoch 44/100, Train Loss: 0.2885, Train Acc: 0.8660, Val Loss: 0.3024, Val Acc: 0.8621\n",
            "Epoch 45/100, Train Loss: 0.2888, Train Acc: 0.8658, Val Loss: 0.3040, Val Acc: 0.8619\n",
            "Epoch 46/100, Train Loss: 0.2882, Train Acc: 0.8670, Val Loss: 0.3040, Val Acc: 0.8592\n",
            "Epoch 47/100, Train Loss: 0.2877, Train Acc: 0.8648, Val Loss: 0.3037, Val Acc: 0.8632\n",
            "Epoch 48/100, Train Loss: 0.2872, Train Acc: 0.8657, Val Loss: 0.3036, Val Acc: 0.8600\n",
            "Epoch 49/100, Train Loss: 0.2876, Train Acc: 0.8659, Val Loss: 0.3041, Val Acc: 0.8610\n",
            "Epoch 50/100, Train Loss: 0.2867, Train Acc: 0.8656, Val Loss: 0.3107, Val Acc: 0.8580\n",
            "Epoch 51/100, Train Loss: 0.2852, Train Acc: 0.8670, Val Loss: 0.3035, Val Acc: 0.8617\n",
            "Epoch 52/100, Train Loss: 0.2848, Train Acc: 0.8667, Val Loss: 0.3030, Val Acc: 0.8631\n",
            "Epoch 53/100, Train Loss: 0.2843, Train Acc: 0.8665, Val Loss: 0.3017, Val Acc: 0.8612\n",
            "Epoch 54/100, Train Loss: 0.2860, Train Acc: 0.8646, Val Loss: 0.3037, Val Acc: 0.8627\n",
            "Epoch 55/100, Train Loss: 0.2843, Train Acc: 0.8679, Val Loss: 0.3079, Val Acc: 0.8627\n",
            "Epoch 56/100, Train Loss: 0.2849, Train Acc: 0.8673, Val Loss: 0.3039, Val Acc: 0.8614\n",
            "Epoch 57/100, Train Loss: 0.2869, Train Acc: 0.8677, Val Loss: 0.3044, Val Acc: 0.8618\n",
            "Epoch 58/100, Train Loss: 0.2841, Train Acc: 0.8683, Val Loss: 0.3062, Val Acc: 0.8614\n",
            "Epoch 59/100, Train Loss: 0.2845, Train Acc: 0.8680, Val Loss: 0.3011, Val Acc: 0.8608\n",
            "Epoch 60/100, Train Loss: 0.2823, Train Acc: 0.8673, Val Loss: 0.3022, Val Acc: 0.8619\n",
            "Epoch 61/100, Train Loss: 0.2833, Train Acc: 0.8666, Val Loss: 0.3059, Val Acc: 0.8631\n",
            "Epoch 62/100, Train Loss: 0.2831, Train Acc: 0.8666, Val Loss: 0.3070, Val Acc: 0.8614\n",
            "Epoch 63/100, Train Loss: 0.2821, Train Acc: 0.8668, Val Loss: 0.3027, Val Acc: 0.8623\n",
            "Epoch 64/100, Train Loss: 0.2803, Train Acc: 0.8680, Val Loss: 0.3058, Val Acc: 0.8628\n",
            "Epoch 65/100, Train Loss: 0.2800, Train Acc: 0.8675, Val Loss: 0.3026, Val Acc: 0.8619\n",
            "Epoch 66/100, Train Loss: 0.2800, Train Acc: 0.8696, Val Loss: 0.3021, Val Acc: 0.8614\n",
            "Epoch 67/100, Train Loss: 0.2795, Train Acc: 0.8689, Val Loss: 0.3071, Val Acc: 0.8618\n",
            "Epoch 68/100, Train Loss: 0.2802, Train Acc: 0.8683, Val Loss: 0.3029, Val Acc: 0.8618\n",
            "Epoch 69/100, Train Loss: 0.2801, Train Acc: 0.8681, Val Loss: 0.3062, Val Acc: 0.8607\n",
            "Epoch 70/100, Train Loss: 0.2794, Train Acc: 0.8692, Val Loss: 0.3032, Val Acc: 0.8608\n",
            "Epoch 71/100, Train Loss: 0.2784, Train Acc: 0.8684, Val Loss: 0.3066, Val Acc: 0.8600\n",
            "Epoch 72/100, Train Loss: 0.2771, Train Acc: 0.8689, Val Loss: 0.3076, Val Acc: 0.8609\n",
            "Epoch 73/100, Train Loss: 0.2780, Train Acc: 0.8687, Val Loss: 0.3072, Val Acc: 0.8626\n",
            "Epoch 74/100, Train Loss: 0.2769, Train Acc: 0.8693, Val Loss: 0.3043, Val Acc: 0.8618\n",
            "Epoch 75/100, Train Loss: 0.2766, Train Acc: 0.8697, Val Loss: 0.3060, Val Acc: 0.8590\n",
            "Early stopping at epoch 75\n",
            "Test Accuracy: 0.8684\n",
            "Test Precision: 0.7324\n",
            "Test Recall: 0.6908\n",
            "Test F1 Score: 0.7110\n",
            "Test AUC: 0.9232\n",
            "Confusion Matrix:\n",
            "[[6901  578]\n",
            " [ 708 1582]]\n",
            "Model saved as ft_piecewise_tuned_adult.pth\n",
            "\n",
            "=== Analyzing PFI vs Attention Correlation for Piecewise Embedding (Tuned) ===\n",
            "Spearman Rank Correlation: 0.7582 (p-value: 0.0017)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-04 17:14:27,591] A new study created in memory with name: no-name-50ce569c-04db-4e5d-ae9b-20e4efde52d4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Tuning Hyperparameters for sparse FT Transformer with Linear Embedding ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-04 17:16:08,736] Trial 0 finished with value: 0.30648740332934166 and parameters: {'d_token': 116, 'num_heads': 6, 'num_layers': 3, 'd_ffn': 206, 'lr': 0.00017869887663842914, 'dropout': 0.16798924000999915, 'n_bins': 25}. Best is trial 0 with value: 0.30648740332934166.\n",
            "[I 2025-03-04 17:17:33,913] Trial 1 finished with value: 0.309349843571263 and parameters: {'d_token': 85, 'num_heads': 7, 'num_layers': 2, 'd_ffn': 238, 'lr': 0.0004646917582706398, 'dropout': 0.24574986745691813, 'n_bins': 51}. Best is trial 0 with value: 0.30648740332934166.\n",
            "[I 2025-03-04 17:18:59,367] Trial 2 finished with value: 0.3199268889523322 and parameters: {'d_token': 43, 'num_heads': 5, 'num_layers': 1, 'd_ffn': 169, 'lr': 0.00020541362096923013, 'dropout': 0.18208680905045538, 'n_bins': 50}. Best is trial 0 with value: 0.30648740332934166.\n",
            "[I 2025-03-04 17:20:55,976] Trial 3 finished with value: 0.3064452827938141 and parameters: {'d_token': 102, 'num_heads': 2, 'num_layers': 2, 'd_ffn': 251, 'lr': 0.00015200441693413884, 'dropout': 0.32072996399966297, 'n_bins': 91}. Best is trial 3 with value: 0.3064452827938141.\n",
            "[I 2025-03-04 17:22:06,537] Trial 4 finished with value: 0.3173704231458326 and parameters: {'d_token': 54, 'num_heads': 3, 'num_layers': 2, 'd_ffn': 225, 'lr': 0.00145900802070638, 'dropout': 0.23592326641867362, 'n_bins': 85}. Best is trial 3 with value: 0.3064452827938141.\n",
            "[I 2025-03-04 17:23:16,990] Trial 5 finished with value: 0.31246559009436636 and parameters: {'d_token': 40, 'num_heads': 3, 'num_layers': 2, 'd_ffn': 109, 'lr': 0.0015446136586606488, 'dropout': 0.15643238300968815, 'n_bins': 41}. Best is trial 3 with value: 0.3064452827938141.\n",
            "[I 2025-03-04 17:24:47,611] Trial 6 finished with value: 0.31075398047124186 and parameters: {'d_token': 52, 'num_heads': 5, 'num_layers': 2, 'd_ffn': 164, 'lr': 0.001161260662932172, 'dropout': 0.07091980678600468, 'n_bins': 99}. Best is trial 3 with value: 0.3064452827938141.\n",
            "[I 2025-03-04 17:25:26,861] Trial 7 finished with value: 0.3370306847556945 and parameters: {'d_token': 92, 'num_heads': 7, 'num_layers': 1, 'd_ffn': 102, 'lr': 0.007829992368537375, 'dropout': 0.18485888718864624, 'n_bins': 72}. Best is trial 3 with value: 0.3064452827938141.\n",
            "[I 2025-03-04 17:27:26,902] Trial 8 finished with value: 0.3078117483565884 and parameters: {'d_token': 40, 'num_heads': 5, 'num_layers': 2, 'd_ffn': 187, 'lr': 0.0009467362352110724, 'dropout': 0.4624447250055862, 'n_bins': 65}. Best is trial 3 with value: 0.3064452827938141.\n",
            "[I 2025-03-04 17:28:29,792] Trial 9 finished with value: 0.3362445824088589 and parameters: {'d_token': 35, 'num_heads': 3, 'num_layers': 1, 'd_ffn': 142, 'lr': 0.004766859792742469, 'dropout': 0.4481402712479795, 'n_bins': 85}. Best is trial 3 with value: 0.3064452827938141.\n",
            "[I 2025-03-04 17:30:07,041] Trial 10 finished with value: 0.30911238611705844 and parameters: {'d_token': 120, 'num_heads': 2, 'num_layers': 3, 'd_ffn': 256, 'lr': 0.00010564469592729071, 'dropout': 0.35476845266732226, 'n_bins': 10}. Best is trial 3 with value: 0.3064452827938141.\n",
            "[I 2025-03-04 17:31:57,854] Trial 11 finished with value: 0.3100827776135937 and parameters: {'d_token': 115, 'num_heads': 8, 'num_layers': 3, 'd_ffn': 217, 'lr': 0.0002376774100540115, 'dropout': 0.34153252212379664, 'n_bins': 20}. Best is trial 3 with value: 0.3064452827938141.\n",
            "[I 2025-03-04 17:33:46,713] Trial 12 finished with value: 0.3064751711583907 and parameters: {'d_token': 104, 'num_heads': 6, 'num_layers': 3, 'd_ffn': 200, 'lr': 0.00010050139699701955, 'dropout': 0.008141880925621348, 'n_bins': 35}. Best is trial 3 with value: 0.3064452827938141.\n",
            "[I 2025-03-04 17:35:11,250] Trial 13 finished with value: 0.3079192121663401 and parameters: {'d_token': 99, 'num_heads': 4, 'num_layers': 3, 'd_ffn': 255, 'lr': 0.00011174768854359257, 'dropout': 0.012210159571568513, 'n_bins': 35}. Best is trial 3 with value: 0.3064452827938141.\n",
            "[I 2025-03-04 17:37:07,128] Trial 14 finished with value: 0.3086686843345242 and parameters: {'d_token': 68, 'num_heads': 6, 'num_layers': 3, 'd_ffn': 197, 'lr': 0.00038876922936281343, 'dropout': 0.3298409013988824, 'n_bins': 100}. Best is trial 3 with value: 0.3064452827938141.\n",
            "[I 2025-03-04 17:38:17,083] Trial 15 finished with value: 0.3172166972391067 and parameters: {'d_token': 103, 'num_heads': 2, 'num_layers': 2, 'd_ffn': 73, 'lr': 0.0004591400030582181, 'dropout': 0.39685563764744847, 'n_bins': 68}. Best is trial 3 with value: 0.3064452827938141.\n",
            "[I 2025-03-04 17:39:52,285] Trial 16 finished with value: 0.30665273075142213 and parameters: {'d_token': 75, 'num_heads': 4, 'num_layers': 2, 'd_ffn': 238, 'lr': 0.0002942123523257406, 'dropout': 0.07991261721318008, 'n_bins': 36}. Best is trial 3 with value: 0.3064452827938141.\n",
            "[I 2025-03-04 17:42:31,758] Trial 17 finished with value: 0.3064639364038744 and parameters: {'d_token': 128, 'num_heads': 8, 'num_layers': 3, 'd_ffn': 135, 'lr': 0.0001321062528714621, 'dropout': 0.29646293748305913, 'n_bins': 60}. Best is trial 3 with value: 0.3064452827938141.\n",
            "[I 2025-03-04 17:43:33,249] Trial 18 finished with value: 0.3237532319561128 and parameters: {'d_token': 127, 'num_heads': 8, 'num_layers': 1, 'd_ffn': 134, 'lr': 0.002631416396066071, 'dropout': 0.29967819688252123, 'n_bins': 83}. Best is trial 3 with value: 0.3064452827938141.\n",
            "[I 2025-03-04 17:44:22,094] Trial 19 finished with value: 0.31684526104119515 and parameters: {'d_token': 107, 'num_heads': 7, 'num_layers': 2, 'd_ffn': 130, 'lr': 0.0008229940404518418, 'dropout': 0.4028544803656182, 'n_bins': 58}. Best is trial 3 with value: 0.3064452827938141.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best trial:\n",
            "  Value (validation loss): 0.3064\n",
            "  Params:\n",
            "    d_token: 102\n",
            "    num_heads: 2\n",
            "    num_layers: 2\n",
            "    d_ffn: 251\n",
            "    lr: 0.00015200441693413884\n",
            "    dropout: 0.32072996399966297\n",
            "    n_bins: 91\n",
            "\n",
            "=== Training sparse FT Transformer with Linear Embedding (Tuned) ===\n",
            "Epoch 1/100, Train Loss: 0.4023, Train Acc: 0.8129, Val Loss: 0.3320, Val Acc: 0.8454\n",
            "Epoch 2/100, Train Loss: 0.3276, Train Acc: 0.8476, Val Loss: 0.3253, Val Acc: 0.8470\n",
            "Epoch 3/100, Train Loss: 0.3188, Train Acc: 0.8516, Val Loss: 0.3166, Val Acc: 0.8502\n",
            "Epoch 4/100, Train Loss: 0.3137, Train Acc: 0.8526, Val Loss: 0.3155, Val Acc: 0.8520\n",
            "Epoch 5/100, Train Loss: 0.3113, Train Acc: 0.8544, Val Loss: 0.3175, Val Acc: 0.8505\n",
            "Epoch 6/100, Train Loss: 0.3109, Train Acc: 0.8541, Val Loss: 0.3132, Val Acc: 0.8522\n",
            "Epoch 7/100, Train Loss: 0.3100, Train Acc: 0.8540, Val Loss: 0.3130, Val Acc: 0.8518\n",
            "Epoch 8/100, Train Loss: 0.3088, Train Acc: 0.8559, Val Loss: 0.3135, Val Acc: 0.8520\n",
            "Epoch 9/100, Train Loss: 0.3083, Train Acc: 0.8559, Val Loss: 0.3098, Val Acc: 0.8531\n",
            "Epoch 10/100, Train Loss: 0.3065, Train Acc: 0.8551, Val Loss: 0.3102, Val Acc: 0.8546\n",
            "Epoch 11/100, Train Loss: 0.3067, Train Acc: 0.8567, Val Loss: 0.3120, Val Acc: 0.8555\n",
            "Epoch 12/100, Train Loss: 0.3050, Train Acc: 0.8566, Val Loss: 0.3108, Val Acc: 0.8535\n",
            "Epoch 13/100, Train Loss: 0.3049, Train Acc: 0.8595, Val Loss: 0.3135, Val Acc: 0.8526\n",
            "Epoch 14/100, Train Loss: 0.3042, Train Acc: 0.8583, Val Loss: 0.3100, Val Acc: 0.8531\n",
            "Epoch 15/100, Train Loss: 0.3029, Train Acc: 0.8584, Val Loss: 0.3120, Val Acc: 0.8528\n",
            "Epoch 16/100, Train Loss: 0.3026, Train Acc: 0.8592, Val Loss: 0.3099, Val Acc: 0.8541\n",
            "Epoch 17/100, Train Loss: 0.3021, Train Acc: 0.8585, Val Loss: 0.3115, Val Acc: 0.8521\n",
            "Epoch 18/100, Train Loss: 0.3006, Train Acc: 0.8591, Val Loss: 0.3127, Val Acc: 0.8545\n",
            "Epoch 19/100, Train Loss: 0.3017, Train Acc: 0.8597, Val Loss: 0.3101, Val Acc: 0.8548\n",
            "Epoch 20/100, Train Loss: 0.3004, Train Acc: 0.8615, Val Loss: 0.3128, Val Acc: 0.8482\n",
            "Epoch 21/100, Train Loss: 0.3002, Train Acc: 0.8584, Val Loss: 0.3122, Val Acc: 0.8564\n",
            "Epoch 22/100, Train Loss: 0.2993, Train Acc: 0.8589, Val Loss: 0.3122, Val Acc: 0.8549\n",
            "Epoch 23/100, Train Loss: 0.2992, Train Acc: 0.8595, Val Loss: 0.3113, Val Acc: 0.8564\n",
            "Epoch 24/100, Train Loss: 0.2985, Train Acc: 0.8614, Val Loss: 0.3109, Val Acc: 0.8544\n",
            "Epoch 25/100, Train Loss: 0.3003, Train Acc: 0.8603, Val Loss: 0.3098, Val Acc: 0.8559\n",
            "Early stopping at epoch 25\n",
            "Test Accuracy: 0.8666\n",
            "Test Precision: 0.7509\n",
            "Test Recall: 0.6450\n",
            "Test F1 Score: 0.6939\n",
            "Test AUC: 0.9196\n",
            "Confusion Matrix:\n",
            "[[6989  490]\n",
            " [ 813 1477]]\n",
            "Model saved as sparse_ft_linear_tuned_adult.pth\n",
            "\n",
            "=== Analyzing PFI vs Attention Correlation for Sparse Linear Embedding (Tuned) ===\n",
            "Spearman Rank Correlation: 0.7670 (p-value: 0.0014)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-04 17:45:15,635] A new study created in memory with name: no-name-d0c09a39-f8e2-41e3-b14c-6f89cf16233d\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Tuning Hyperparameters for sparse FT Transformer with Piecewise Linear Embedding ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-03-04 17:54:46,504] Trial 0 finished with value: 0.4499333832533129 and parameters: {'d_token': 58, 'num_heads': 4, 'num_layers': 2, 'd_ffn': 160, 'lr': 0.009929135345980139, 'dropout': 0.11062714321262168, 'n_bins': 71}. Best is trial 0 with value: 0.4499333832533129.\n",
            "[I 2025-03-04 17:58:42,982] Trial 1 finished with value: 0.3146006738947284 and parameters: {'d_token': 58, 'num_heads': 7, 'num_layers': 1, 'd_ffn': 188, 'lr': 0.00041847257551783004, 'dropout': 0.2717393676873803, 'n_bins': 19}. Best is trial 1 with value: 0.3146006738947284.\n",
            "[I 2025-03-04 18:03:00,761] Trial 2 finished with value: 0.37796270342603805 and parameters: {'d_token': 52, 'num_heads': 7, 'num_layers': 3, 'd_ffn': 206, 'lr': 0.0037844740333583646, 'dropout': 0.31220989812613237, 'n_bins': 62}. Best is trial 1 with value: 0.3146006738947284.\n",
            "[I 2025-03-04 18:16:14,418] Trial 3 finished with value: 0.31273629468294883 and parameters: {'d_token': 69, 'num_heads': 5, 'num_layers': 1, 'd_ffn': 250, 'lr': 0.0002958999543643145, 'dropout': 0.05763815895475838, 'n_bins': 91}. Best is trial 3 with value: 0.31273629468294883.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best trial:\n",
            "  Value (validation loss): 0.3127\n",
            "  Params:\n",
            "    d_token: 69\n",
            "    num_heads: 5\n",
            "    num_layers: 1\n",
            "    d_ffn: 250\n",
            "    lr: 0.0002958999543643145\n",
            "    dropout: 0.05763815895475838\n",
            "    n_bins: 91\n",
            "\n",
            "=== Training sparse FT Transformer with Piecewise Linear Embedding (Tuned) ===\n",
            "Epoch 1/100, Train Loss: 0.4285, Train Acc: 0.7863, Val Loss: 0.3522, Val Acc: 0.8343\n",
            "Epoch 2/100, Train Loss: 0.3386, Train Acc: 0.8420, Val Loss: 0.3430, Val Acc: 0.8338\n",
            "Epoch 3/100, Train Loss: 0.3314, Train Acc: 0.8473, Val Loss: 0.3367, Val Acc: 0.8462\n",
            "Epoch 4/100, Train Loss: 0.3255, Train Acc: 0.8498, Val Loss: 0.3275, Val Acc: 0.8502\n",
            "Epoch 5/100, Train Loss: 0.3219, Train Acc: 0.8507, Val Loss: 0.3269, Val Acc: 0.8507\n",
            "Epoch 6/100, Train Loss: 0.3188, Train Acc: 0.8515, Val Loss: 0.3260, Val Acc: 0.8495\n",
            "Epoch 7/100, Train Loss: 0.3180, Train Acc: 0.8518, Val Loss: 0.3235, Val Acc: 0.8518\n",
            "Epoch 8/100, Train Loss: 0.3172, Train Acc: 0.8528, Val Loss: 0.3324, Val Acc: 0.8424\n",
            "Epoch 9/100, Train Loss: 0.3152, Train Acc: 0.8551, Val Loss: 0.3316, Val Acc: 0.8424\n",
            "Epoch 10/100, Train Loss: 0.3150, Train Acc: 0.8534, Val Loss: 0.3226, Val Acc: 0.8522\n",
            "Epoch 11/100, Train Loss: 0.3127, Train Acc: 0.8544, Val Loss: 0.3222, Val Acc: 0.8513\n",
            "Epoch 12/100, Train Loss: 0.3136, Train Acc: 0.8547, Val Loss: 0.3228, Val Acc: 0.8489\n",
            "Epoch 13/100, Train Loss: 0.3125, Train Acc: 0.8557, Val Loss: 0.3247, Val Acc: 0.8499\n",
            "Epoch 14/100, Train Loss: 0.3110, Train Acc: 0.8532, Val Loss: 0.3351, Val Acc: 0.8468\n",
            "Epoch 15/100, Train Loss: 0.3118, Train Acc: 0.8543, Val Loss: 0.3258, Val Acc: 0.8491\n",
            "Epoch 16/100, Train Loss: 0.3124, Train Acc: 0.8541, Val Loss: 0.3267, Val Acc: 0.8459\n",
            "Epoch 17/100, Train Loss: 0.3098, Train Acc: 0.8557, Val Loss: 0.3217, Val Acc: 0.8521\n",
            "Epoch 18/100, Train Loss: 0.3098, Train Acc: 0.8557, Val Loss: 0.3211, Val Acc: 0.8522\n",
            "Epoch 19/100, Train Loss: 0.3077, Train Acc: 0.8562, Val Loss: 0.3177, Val Acc: 0.8539\n",
            "Epoch 20/100, Train Loss: 0.3048, Train Acc: 0.8584, Val Loss: 0.3155, Val Acc: 0.8536\n",
            "Epoch 21/100, Train Loss: 0.3039, Train Acc: 0.8572, Val Loss: 0.3185, Val Acc: 0.8540\n",
            "Epoch 22/100, Train Loss: 0.3045, Train Acc: 0.8576, Val Loss: 0.3160, Val Acc: 0.8568\n",
            "Epoch 23/100, Train Loss: 0.3037, Train Acc: 0.8584, Val Loss: 0.3150, Val Acc: 0.8532\n",
            "Epoch 24/100, Train Loss: 0.3060, Train Acc: 0.8564, Val Loss: 0.3174, Val Acc: 0.8572\n",
            "Epoch 25/100, Train Loss: 0.3060, Train Acc: 0.8564, Val Loss: 0.3162, Val Acc: 0.8539\n",
            "Epoch 26/100, Train Loss: 0.3021, Train Acc: 0.8593, Val Loss: 0.3135, Val Acc: 0.8555\n",
            "Epoch 27/100, Train Loss: 0.3021, Train Acc: 0.8592, Val Loss: 0.3155, Val Acc: 0.8557\n",
            "Epoch 28/100, Train Loss: 0.3022, Train Acc: 0.8584, Val Loss: 0.3134, Val Acc: 0.8555\n",
            "Epoch 29/100, Train Loss: 0.3016, Train Acc: 0.8588, Val Loss: 0.3220, Val Acc: 0.8508\n",
            "Epoch 30/100, Train Loss: 0.3021, Train Acc: 0.8583, Val Loss: 0.3188, Val Acc: 0.8557\n",
            "Epoch 31/100, Train Loss: 0.3014, Train Acc: 0.8590, Val Loss: 0.3156, Val Acc: 0.8559\n",
            "Epoch 32/100, Train Loss: 0.3006, Train Acc: 0.8586, Val Loss: 0.3150, Val Acc: 0.8554\n",
            "Epoch 33/100, Train Loss: 0.2986, Train Acc: 0.8597, Val Loss: 0.3190, Val Acc: 0.8569\n",
            "Epoch 34/100, Train Loss: 0.3003, Train Acc: 0.8590, Val Loss: 0.3177, Val Acc: 0.8523\n",
            "Epoch 35/100, Train Loss: 0.2984, Train Acc: 0.8592, Val Loss: 0.3151, Val Acc: 0.8575\n",
            "Epoch 36/100, Train Loss: 0.2995, Train Acc: 0.8586, Val Loss: 0.3157, Val Acc: 0.8553\n",
            "Epoch 37/100, Train Loss: 0.2994, Train Acc: 0.8613, Val Loss: 0.3146, Val Acc: 0.8559\n",
            "Epoch 38/100, Train Loss: 0.2989, Train Acc: 0.8587, Val Loss: 0.3151, Val Acc: 0.8553\n",
            "Epoch 39/100, Train Loss: 0.2994, Train Acc: 0.8603, Val Loss: 0.3166, Val Acc: 0.8548\n",
            "Epoch 40/100, Train Loss: 0.2979, Train Acc: 0.8604, Val Loss: 0.3181, Val Acc: 0.8560\n",
            "Epoch 41/100, Train Loss: 0.2983, Train Acc: 0.8598, Val Loss: 0.3158, Val Acc: 0.8566\n",
            "Epoch 42/100, Train Loss: 0.2959, Train Acc: 0.8608, Val Loss: 0.3155, Val Acc: 0.8571\n",
            "Epoch 43/100, Train Loss: 0.2983, Train Acc: 0.8603, Val Loss: 0.3141, Val Acc: 0.8577\n",
            "Epoch 44/100, Train Loss: 0.2968, Train Acc: 0.8594, Val Loss: 0.3167, Val Acc: 0.8548\n",
            "Early stopping at epoch 44\n",
            "Test Accuracy: 0.8657\n",
            "Test Precision: 0.7720\n",
            "Test Recall: 0.6061\n",
            "Test F1 Score: 0.6791\n",
            "Test AUC: 0.9168\n",
            "Confusion Matrix:\n",
            "[[7069  410]\n",
            " [ 902 1388]]\n",
            "Model saved as sparse_ft_piecewise_tuned_adult.pth\n",
            "\n",
            "=== Analyzing PFI vs Attention Correlation for Sparse Piecewise Embedding (Tuned) ===\n",
            "Spearman Rank Correlation: 0.7626 (p-value: 0.0015)\n",
            "\n",
            "=== Comparison of Tuned Models ===\n",
            "FT Transformer (Linear Tuned): Accuracy=0.8705, F1=0.7046\n",
            "FT Transformer (Piecewise Tuned): Accuracy=0.8684, F1=0.7110\n",
            "Sparse FT Transformer (Linear Tuned): Accuracy=0.8666, F1=0.6939\n",
            "Sparse FT Transformer (Piecewise Tuned): Accuracy=0.8657, F1=0.6791\n",
            "\n",
            "=== Comparison of PFI-Attention Correlations (Tuned Models) ===\n",
            "FT Transformer (Linear Tuned): ρ=0.2659, p-value=0.3581\n",
            "FT Transformer (Piecewise Tuned): ρ=0.7582, p-value=0.0017\n",
            "Sparse FT Transformer (Linear Tuned): ρ=0.7670, p-value=0.0014\n",
            "Sparse FT Transformer (Piecewise Tuned): ρ=0.7626, p-value=0.0015\n",
            "\n",
            "Visualizations saved as 'model_comparison_adult.png' and 'feature_importance_comparison_adult.png'\n",
            "Results saved as results_adult.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-YCEi0O8cest"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}